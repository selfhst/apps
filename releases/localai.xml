<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <id>https://selfh.st/apps</id>
  <title>LocalAI Releases</title>
  <updated>2025-08-01T06:36:38.834978-04:00</updated>
  <author>
    <name>selfh.st</name>
    <email>contact@selfh.st</email>
  </author>
  <link href="https://selfh.st/apps" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="0.9.0">python-feedgen</generator>
  <subtitle>Self-hosted software releases generated by selfh.st</subtitle>
  <entry>
    <id>https://github.com/mudler/LocalAI/releases/tag/v3.2.0</id>
    <title>New release for LocalAI: v3.2.0</title>
    <updated>2025-07-24T16:36:41-04:00</updated>
    <author>
      <name>mudler/LocalAI</name>
    </author>
    <content>&lt;h1 align="center"&gt;
  &lt;br&gt;
  &lt;img height="300" src="https://raw.githubusercontent.com/mudler/LocalAI/refs/heads/master/core/http/static/logo.png"&gt; &lt;br&gt;
&lt;br&gt;
üöÄ LocalAI 3.2.0
&lt;/h1&gt;

&lt;p&gt;Welcome to LocalAI 3.2.0! This is a release that refactors our architecture to be more flexible and lightweight. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The core is now separated from all the backends&lt;/strong&gt;, making LocalAI faster to download, easier to manage, portable, and much more smaller.&lt;/p&gt;
&lt;h2&gt;TL;DR ‚Äì What‚Äôs New in LocalAI 3.2.0 üéâ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;üß© Modular Backends: All backends now live outside the main binary in our new Backend Gallery. This means you can update, add, or manage backends independently of LocalAI releases.&lt;/li&gt;
&lt;li&gt;üìâ Leaner Than Ever: The LocalAI binary and container images are drastically smaller, making for faster downloads and a reduced footprint.&lt;/li&gt;
&lt;li&gt;ü§ñ Smart Backend Installation: It just works! When you install a model, LocalAI automatically detects your hardware (CPU, NVIDIA, AMD, Intel) and downloads the necessary backend. No more manual configuration!&lt;/li&gt;
&lt;li&gt;üõ†Ô∏è Simplified Build Process: The new modular architecture significantly simplifies the build process for contributors and power users.&lt;/li&gt;
&lt;li&gt;‚ö°Ô∏è Intel GPU Support for Whisper: Transcription with Whisper can now be accelerated on Intel GPUs using SYCL, bringing more hardware options to our users.&lt;/li&gt;
&lt;li&gt;üó£Ô∏è Enhanced Realtime Audio: We've added speech started and stopped events for more interactive applications and OpenAI-compatible support for the input_audio field in the chat API.&lt;/li&gt;
&lt;li&gt;üß† Massive Model Expansion: The gallery has been updated with over 50 new models, including the latest from &lt;code&gt;Qwen3&lt;/code&gt;, &lt;code&gt;Gemma&lt;/code&gt;, &lt;code&gt;Mistral&lt;/code&gt;, &lt;code&gt;Nemotron&lt;/code&gt;, and more!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: CI is in the process of building all the backends for this release and will be available soon - if you hit any issue, please try in a few, thanks for understanding!
&lt;strong&gt;Note&lt;/strong&gt;: Some parts of the documentation and the installation scripts (that download the release binaries) have to yet be adapted to the latest changes and/or might not reflect the current state&lt;/p&gt;
&lt;h2&gt;A New Modular Architecture üß©&lt;/h2&gt;
&lt;p&gt;The biggest change in v3.2.0 is the complete separation of inference backends from the core LocalAI binary. Backends like llama.cpp, whisper.cpp, piper, and stablediffusion-ggml are no longer bundled in.&lt;/p&gt;
&lt;p&gt;This fundamental shift makes LocalAI:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lighter: Significantly smaller binary and container image sizes.&lt;/li&gt;
&lt;li&gt;More Flexible: Update backends anytime from the gallery without waiting for a new LocalAI release.&lt;/li&gt;
&lt;li&gt;Easier to Maintain: A cleaner, more streamlined codebase for faster development.&lt;/li&gt;
&lt;li&gt;Easier to Customize: you can build your own backends and install them in your LocalAI instances.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Smart, Automatic Backend Installation ü§ñ&lt;/h2&gt;
&lt;p&gt;To make the new modular system seamless, LocalAI now features automatic backend installation.&lt;/p&gt;
&lt;p&gt;When you install a model from the gallery (or a YAML file), LocalAI intelligently detects the required backend and your system's capabilities, then downloads the correct version for you. Whether you're running on a standard CPU, an NVIDIA GPU, an AMD GPU, or an Intel GPU, LocalAI handles it automatically.&lt;/p&gt;
&lt;p&gt;For advanced use cases or to override auto-detection, you can use the LOCALAI_FORCE_META_BACKEND_CAPABILITY environment variable. Here are the available options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;default&lt;/code&gt;: Forces CPU-only backend. This is the fallback if no specific hardware is detected.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nvidia&lt;/code&gt;: Forces backends compiled with CUDA support for NVIDIA GPUs.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;amd&lt;/code&gt;: Forces backends compiled with ROCm support for AMD GPUs.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;intel&lt;/code&gt;: Forces backends compiled with SYCL/oneAPI support for Intel GPUs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;The Backend Gallery &amp;amp; CLI Control üñºÔ∏è&lt;/h2&gt;
&lt;p&gt;You are in full control. You can browse, install, and manage all available backends directly from the WebUI or using the new CLI commands:&lt;/p&gt;
&lt;p&gt;```bash&lt;/p&gt;
&lt;h1&gt;List all available backends in the gallery&lt;/h1&gt;
&lt;p&gt;local-ai backends list&lt;/p&gt;
&lt;h1&gt;Install a specific backend (e.g., llama-cpp)&lt;/h1&gt;
&lt;p&gt;local-ai backends install llama-cpp&lt;/p&gt;
&lt;h1&gt;Uninstall a backend&lt;/h1&gt;
&lt;p&gt;local-ai backends uninstall llama-cpp
```&lt;/p&gt;
&lt;p&gt;For development, offline or air-gapped environments, you can now also install backends directly from a local OCI tar file:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;local-ai backends install "ocifile://&amp;lt;PATH_TO_TAR_FILE&amp;gt;"&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;Other Key Improvements&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;üó£Ô∏è Enhanced Realtime and Audio APIs: Building voice-activated applications is now easier.&lt;/li&gt;
&lt;li&gt;The new speech started and stopped events give you precise control over realtime audio streams.&lt;/li&gt;
&lt;li&gt;We now support the input_audio field in the /v1/chat/completions endpoint for multimodal audio inputs, improving OpenAI compatibility.&lt;/li&gt;
&lt;li&gt;‚ö°Ô∏è Intel GPU Acceleration for Whisper: Our Whisper backend now supports SYCL, enabling hardware-accelerated transcriptions on Intel GPUs.&lt;/li&gt;
&lt;li&gt;‚úÖ UI and Bug Fixes: We've squashed several bugs for a smoother experience, including a fix that correctly shows the download status for backend images in the gallery, so you always know what's happening.&lt;/li&gt;
&lt;li&gt;üß† Massive Model Gallery Expansion: Our model gallery has never been bigger! We've added over 50 new and updated models, with a focus on powerful new releases like qwen3, devstral-small, and nemotron.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;üö® Important Note for Upgrading&lt;/h2&gt;
&lt;p&gt;Due to the new modular architecture, if you have existing models installed with a version prior to 3.2.0, they might not have a specific backend assigned.&lt;/p&gt;
&lt;p&gt;After upgrading, you may need to install the required backend manually for these models to work. You can do this easily from the WebUI or via the CLI: &lt;code&gt;local-ai backends install &amp;lt;backend_name&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;The Complete Local Stack for Privacy-First AI&lt;/h2&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td width="30%" valign="top" align="center"&gt;
      &lt;a href="https://github.com/mudler/LocalAI"&gt;
        &lt;img src="https://raw.githubusercontent.com/mudler/LocalAI/refs/heads/master/core/http/static/logo.png" width="200" alt="LocalAI Logo"&gt;
        &lt;h3&gt;LocalAI&lt;/h3&gt;
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td width="70%" valign="top"&gt;
      &lt;p&gt;The free, Open Source OpenAI alternative. Acts as a drop-in replacement REST API compatible with OpenAI specifications for local AI inferencing. No GPU required.&lt;/p&gt;
      &lt;p&gt;&lt;em&gt;Link:&lt;/em&gt; &lt;a href="https://github.com/mudler/LocalAI"&gt;https://github.com/mudler/LocalAI&lt;/a&gt;&lt;/p&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width="30%" valign="top" align="center"&gt;
      &lt;a href="https://github.com/mudler/LocalAGI"&gt;
         &lt;img src="https://raw.githubusercontent.com/mudler/LocalAGI/refs/heads/main/webui/react-ui/public/logo_2.png" width="200" alt="LocalAGI Logo"&gt;
         &lt;h3&gt;LocalAGI&lt;/h3&gt;
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td width="70%" valign="top"&gt;
      &lt;p&gt;A powerful Local AI agent management platform. Serves as a drop-in replacement for OpenAI's Responses API, supercharged with advanced agentic capabilities and a no-code UI.&lt;/p&gt;
      &lt;p&gt;&lt;em&gt;Link:&lt;/em&gt; &lt;a href="https://github.com/mudler/LocalAGI"&gt;https://github.com/mudler/LocalAGI&lt;/a&gt;&lt;/p&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width="30%" valign="top" align="center"&gt;
      &lt;a href="https://github.com/mudler/LocalRecall"&gt;
         &lt;img src="https://raw.githubusercontent.com/mudler/LocalRecall/refs/heads/main/static/localrecall_horizontal.png" width="200" alt="LocalRecall Logo"&gt;
         &lt;h3&gt;LocalRecall&lt;/h3&gt;
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td width="70%" valign="top"&gt;
      &lt;p&gt;A RESTful API and knowledge base management system providing persistent memory and storage capabilities for AI agents. Designed to work alongside LocalAI and LocalAGI.&lt;/p&gt;
      &lt;p&gt;&lt;em&gt;Link:&lt;/em&gt; &lt;a href="https://github.com/mudler/LocalRecall"&gt;https://github.com/mudler/LocalRecall&lt;/a&gt;&lt;/p&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h2&gt;Thank you! ‚ù§Ô∏è&lt;/h2&gt;
&lt;p&gt;A massive &lt;strong&gt;THANK YOU&lt;/strong&gt; to our incredible community and our sponsors! LocalAI has over &lt;strong&gt;34,100 stars&lt;/strong&gt;, and LocalAGI has already rocketed past &lt;strong&gt;900+ stars&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;As a reminder, LocalAI is real FOSS (Free and Open Source Software) and its sibling projects are community-driven and not backed by VCs or a company. We rely on contributors donating their spare time and our sponsors to provide us the hardware! If you love open-source, privacy-first AI, please consider starring the repos, contributing code, reporting bugs, or spreading the word!&lt;/p&gt;
&lt;p&gt;üëâ &lt;strong&gt;Check out the reborn LocalAGI v2 today:&lt;/strong&gt; &lt;a href="https://github.com/mudler/LocalAGI"&gt;https://github.com/mudler/LocalAGI&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Full changelog :point_down:&lt;/h2&gt;
&lt;details&gt;

&lt;summary&gt;
:point_right: Click to expand :point_left: 
&lt;/summary&gt;


## What's Changed
### Breaking Changes üõ†
* feat: do not bundle llama-cpp anymore by @mudler in https://github.com/mudler/LocalAI/pull/5790
* feat: refactor build process, drop embedded backends by @mudler in https://github.com/mudler/LocalAI/pull/5875
### Bug fixes :bug:
* fix(gallery): automatically install model from name by @mudler in https://github.com/mudler/LocalAI/pull/5757
* fix: Diffusers and XPU fixes by @richiejp in https://github.com/mudler/LocalAI/pull/5737
* fix(gallery): correctly show status for downloading OCI images by @mudler in https://github.com/mudler/LocalAI/pull/5774
* fix: explorer page should not have login by @mudler in https://github.com/mudler/LocalAI/pull/5855
* fix: dockerfile typo by @LeonSijiaLu in https://github.com/mudler/LocalAI/pull/5823
* fix(docs): Resolve logo overlap on tablet view by @dedyf5 in https://github.com/mudler/LocalAI/pull/5853
* fix: do not pass by environ to ffmpeg by @mudler in https://github.com/mudler/LocalAI/pull/5871
* fix(p2p): adapt to backend changes, general improvements by @mudler in https://github.com/mudler/LocalAI/pull/5889
### Exciting New Features üéâ
* feat(llama.cpp): allow to set kv-overrides by @mudler in https://github.com/mudler/LocalAI/pull/5745
* feat(backends): add metas in the gallery by @mudler in https://github.com/mudler/LocalAI/pull/5784
* feat(system): detect and allow to override capabilities by @mudler in https://github.com/mudler/LocalAI/pull/5785
* chore(cli): add backends CLI to manipulate and install backends by @mudler in https://github.com/mudler/LocalAI/pull/5787
* feat(whisper): Enable SYCL by @richiejp in https://github.com/mudler/LocalAI/pull/5802
* feat(cli): allow to install backends from OCI tar files by @mudler in https://github.com/mudler/LocalAI/pull/5816
* feat(cli): add command to create custom OCI images from directories by @mudler in https://github.com/mudler/LocalAI/pull/5844
* feat(realtime): Add speech started and stopped events by @richiejp in https://github.com/mudler/LocalAI/pull/5856
* fix: autoload backends when installing models from YAML files by @mudler in https://github.com/mudler/LocalAI/pull/5859
* feat: split piper from main binary by @mudler in https://github.com/mudler/LocalAI/pull/5858
* feat: remove stablediffusion-ggml from main binary by @mudler in https://github.com/mudler/LocalAI/pull/5861
* feat: split whisper from main binary by @mudler in https://github.com/mudler/LocalAI/pull/5863
* feat(openai): support input_audio chat api field by @mgoltzsche in https://github.com/mudler/LocalAI/pull/5870
* fix(realtime): Reset speech started flag on commit by @richiejp in https://github.com/mudler/LocalAI/pull/5879
* fix(build): Add and update ONEAPI_VERSION by @richiejp in https://github.com/mudler/LocalAI/pull/5874
### üß† Models
* chore(model gallery): add qwen3-55b-a3b-total-recall-v1.3-i1 by @mudler in https://github.com/mudler/LocalAI/pull/5746
* chore(model gallery): add qwen3-55b-a3b-total-recall-deep-40x by @mudler in https://github.com/mudler/LocalAI/pull/5747
* chore(model gallery): add qwen3-42b-a3b-stranger-thoughts-deep20x-abliterated-uncensored-i1 by @mudler in https://github.com/mudler/LocalAI/pull/5748
* chore(model gallery): add mistral-small-3.2-46b-the-brilliant-raconteur-ii-instruct-2506 by @mudler in https://github.com/mudler/LocalAI/pull/5749
* chore(model gallery): add qwen3-22b-a3b-the-harley-quinn by @mudler in https://github.com/mudler/LocalAI/pull/5750
* chore(model gallery): add gemma-3-4b-it-max-horror-uncensored-dbl-x-imatrix by @mudler in https://github.com/mudler/LocalAI/pull/5751
* chore(model gallery): add qwen3-33b-a3b-stranger-thoughts-abliterated-uncensored by @mudler in https://github.com/mudler/LocalAI/pull/5755
* chore(model gallery): add thedrummer_anubis-70b-v1.1 by @mudler in https://github.com/mudler/LocalAI/pull/5771
* chore(model gallery): add steelskull_l3.3-shakudo-70b by @mudler in https://github.com/mudler/LocalAI/pull/5772
* chore(model gallery): add pinkpixel_crystal-think-v2 by @mudler in https://github.com/mudler/LocalAI/pull/5773
* chore(model gallery): add helpingai_dhanishtha-2.0-preview by @mudler in https://github.com/mudler/LocalAI/pull/5791
* chore(model gallery): add agentica-org_deepswe-preview by @mudler in https://github.com/mudler/LocalAI/pull/5792
* chore(model gallery): add zerofata_ms3.2-paintedfantasy-visage-33b by @mudler in https://github.com/mudler/LocalAI/pull/5793
* chore(model gallery): add ockerman0_anubislemonade-70b-v1 by @mudler in https://github.com/mudler/LocalAI/pull/5794
* chore(model gallery): add sicariussicariistuff_impish_llama_4b by @mudler in https://github.com/mudler/LocalAI/pull/5799
* chore(model gallery): add nano_imp_1b-q8_0 by @mudler in https://github.com/mudler/LocalAI/pull/5800
* chore(model gallery): add compumacy-experimental-32b by @mudler in https://github.com/mudler/LocalAI/pull/5803
* chore(model gallery): add mini-hydra by @mudler in https://github.com/mudler/LocalAI/pull/5804
* chore(model gallery): add zonui-3b-i1 by @mudler in https://github.com/mudler/LocalAI/pull/5805
* chore(model gallery): add huihui-jan-nano-abliterated by @mudler in https://github.com/mudler/LocalAI/pull/5806
* chore(model gallery): add cognitivecomputations_dolphin-mistral-24b-venice-edition by @mudler in https://github.com/mudler/LocalAI/pull/5813
* chore(model gallery): add ockerman0_anubislemonade-70b-v1.1 by @mudler in https://github.com/mudler/LocalAI/pull/5814
* chore(model gallery): add qwen3-8b-shiningvaliant3 by @mudler in https://github.com/mudler/LocalAI/pull/5815
* chore(model gallery): add lyranovaheart_starfallen-snow-fantasy-24b-ms3.2-v0.0 by @mudler in https://github.com/mudler/LocalAI/pull/5818
* chore(model gallery): add zerofata_l3.3-geneticlemonade-opus-70b by @mudler in https://github.com/mudler/LocalAI/pull/5819
* chore(model gallery): add huggingfacetb_smollm3-3b by @mudler in https://github.com/mudler/LocalAI/pull/5820
* chore(model gallery): add delta-vector_plesio-70b by @mudler in https://github.com/mudler/LocalAI/pull/5825
* chore(model gallery): add thedrummer_big-tiger-gemma-27b-v3 by @mudler in https://github.com/mudler/LocalAI/pull/5826
* chore(model gallery): add thedrummer_tiger-gemma-12b-v3 by @mudler in https://github.com/mudler/LocalAI/pull/5827
* chore(model gallery): add microsoft_nextcoder-32b by @mudler in https://github.com/mudler/LocalAI/pull/5832
* chore(model gallery): add huihui-ai_huihui-gemma-3n-e4b-it-abliterated by @mudler in https://github.com/mudler/LocalAI/pull/5833
* chore(model gallery): add mistralai_devstral-small-2507 by @mudler in https://github.com/mudler/LocalAI/pull/5834
* chore(model gallery): add nvidia_llama-3_3-nemotron-super-49b-genrm-multilingual by @mudler in https://github.com/mudler/LocalAI/pull/5837
* chore(model gallery): add mistral-2x24b-moe-power-coder-magistral-devstral-reasoning-ultimate-neo-max-44b by @mudler in https://github.com/mudler/LocalAI/pull/5838
* chore(model gallery): add impish_magic_24b-i1 by @mudler in https://github.com/mudler/LocalAI/pull/5839
* chore(model gallery): add google_medgemma-4b-it by @mudler in https://github.com/mudler/LocalAI/pull/5842
* chore(model gallery): add google_medgemma-27b-it by @mudler in https://github.com/mudler/LocalAI/pull/5843
* chore(model gallery): add zhi-create-qwen3-32b-i1 by @mudler in https://github.com/mudler/LocalAI/pull/5847
* chore(model gallery): add sophosympatheia_strawberrylemonade-70b-v1.1 by @mudler in https://github.com/mudler/LocalAI/pull/5848
* chore(model-gallery): :arrow_up: update checksum by @localai-bot in https://github.com/mudler/LocalAI/pull/5865
* chore(model gallery): add omega-qwen3-atom-8b by @mudler in https://github.com/mudler/LocalAI/pull/5883
* chore(model gallery): add dream-org_dream-v0-instruct-7b by @mudler in https://github.com/mudler/LocalAI/pull/5884
* chore(model gallery): add entfane_math-genius-7b by @mudler in https://github.com/mudler/LocalAI/pull/5885
* chore(model gallery): add menlo_lucy by @mudler in https://github.com/mudler/LocalAI/pull/5886
* chore(model gallery): add qwen3-235b-a22b-instruct-2507 by @mudler in https://github.com/mudler/LocalAI/pull/5887
* chore(model gallery): add qwen3-coder-480b-a35b-instruct by @mudler in https://github.com/mudler/LocalAI/pull/5888
### üìñ Documentation and examples
* fix(docs): Improve Header Responsiveness - Hide "Star us on GitHub!" on Mobile by @dedyf5 in https://github.com/mudler/LocalAI/pull/5770
### üëí Dependencies
* chore: :arrow_up: Update ggml-org/llama.cpp to `27208bf657cfe7262791df473927225e48efe482` by @localai-bot in https://github.com/mudler/LocalAI/pull/5753
* chore: :arrow_up: Update ggml-org/llama.cpp to `caf5681fcb47dfe9bafee94ef9aa8f669ac986c7` by @localai-bot in https://github.com/mudler/LocalAI/pull/5758
* chore: :arrow_up: Update ggml-org/llama.cpp to `0a5a3b5cdfd887cf0f8e09d9ff89dee130cfcdde` by @localai-bot in https://github.com/mudler/LocalAI/pull/5759
* chore: :arrow_up: Update ggml-org/whisper.cpp to `bca021c9740b267c2973fba56555be052006023a` by @localai-bot in https://github.com/mudler/LocalAI/pull/5776
* chore: :arrow_up: Update ggml-org/llama.cpp to `de569441470332ff922c23fb0413cc957be75b25` by @localai-bot in https://github.com/mudler/LocalAI/pull/5777
* chore: :arrow_up: Update ggml-org/whisper.cpp to `d9999d54c868b8bfcd376aa26067e787d53e679e` by @localai-bot in https://github.com/mudler/LocalAI/pull/5782
* chore: :arrow_up: Update ggml-org/llama.cpp to `e75ba4c0434eb759eb7ff74e034ebe729053e575` by @localai-bot in https://github.com/mudler/LocalAI/pull/5783
* chore(bark-cpp): generalize and move to bark-cpp by @mudler in https://github.com/mudler/LocalAI/pull/5786
* chore: :arrow_up: Update PABannier/bark.cpp to `5d5be84f089ab9ea53b7a793f088d3fbf7247495` by @localai-bot in https://github.com/mudler/LocalAI/pull/4786
* chore: :arrow_up: Update ggml-org/llama.cpp to `bee28421be25fd447f61cb6db64d556cbfce32ec` by @localai-bot in https://github.com/mudler/LocalAI/pull/5788
* chore: :arrow_up: Update ggml-org/llama.cpp to `ef797db357e44ecb7437fa9d22f4e1614104b342` by @localai-bot in https://github.com/mudler/LocalAI/pull/5795
* chore: :arrow_up: Update ggml-org/llama.cpp to `a0374a67e2924f2e845cdc59dd67d9a44065a89c` by @localai-bot in https://github.com/mudler/LocalAI/pull/5798
* chore: :arrow_up: Update ggml-org/llama.cpp to `6491d6e4f1caf0ad2221865b4249ae6938a6308c` by @localai-bot in https://github.com/mudler/LocalAI/pull/5801
* chore: :arrow_up: Update ggml-org/llama.cpp to `12f55c302b35cfe900b84c5fe67c262026af9c44` by @localai-bot in https://github.com/mudler/LocalAI/pull/5808
* chore: :arrow_up: Update ggml-org/whisper.cpp to `869335f2d58d04010535be9ae23a69a9da12a169` by @localai-bot in https://github.com/mudler/LocalAI/pull/5809
* chore: :arrow_up: Update ggml-org/llama.cpp to `6efcd65945a98cf6883cdd9de4c8ccd8c79d219a` by @localai-bot in https://github.com/mudler/LocalAI/pull/5817
* chore: :arrow_up: Update ggml-org/llama.cpp to `0b8855775c6b873931d40b77a5e42558aacbde52` by @localai-bot in https://github.com/mudler/LocalAI/pull/5830
* chore: :arrow_up: Update ggml-org/llama.cpp to `f5e96b368f1acc7f53c390001b936517c4d18999` by @localai-bot in https://github.com/mudler/LocalAI/pull/5835
* chore: :arrow_up: Update ggml-org/llama.cpp to `c31e60647def83d671bac5ab5b35579bf25d9aa1` by @localai-bot in https://github.com/mudler/LocalAI/pull/5840
* chore: :arrow_up: Update ggml-org/whisper.cpp to `3775c503d5133d3d8b99d7d062e87a54064b0eb8` by @localai-bot in https://github.com/mudler/LocalAI/pull/5841
* chore: :arrow_up: Update ggml-org/whisper.cpp to `a16da91365700f396da916d16a7f5a2ec99364b9` by @localai-bot in https://github.com/mudler/LocalAI/pull/5846
* chore: :arrow_up: Update ggml-org/llama.cpp to `982e347255723fe6d02e60ee30cfdd0559c884c5` by @localai-bot in https://github.com/mudler/LocalAI/pull/5845
* chore: :arrow_up: Update ggml-org/whisper.cpp to `032697b9a850dc2615555e2a93a683cc3dd58559` by @localai-bot in https://github.com/mudler/LocalAI/pull/5849
* chore: :arrow_up: Update ggml-org/llama.cpp to `bdca38376f7e8dd928defe01ce6a16218a64b040` by @localai-bot in https://github.com/mudler/LocalAI/pull/5850
* chore: :arrow_up: Update ggml-org/llama.cpp to `4a4f426944e79b79e389f9ed7b34831cb9b637ad` by @localai-bot in https://github.com/mudler/LocalAI/pull/5852
* chore: :arrow_up: Update ggml-org/llama.cpp to `496957e1cbcb522abc63aa18521036e40efce985` by @localai-bot in https://github.com/mudler/LocalAI/pull/5854
* chore: :arrow_up: Update ggml-org/llama.cpp to `d6fb3f6b49b27ef1c0f4cf5128e041f7e7dc03af` by @localai-bot in https://github.com/mudler/LocalAI/pull/5857
* chore(deps): bump securego/gosec from 2.22.5 to 2.22.7 by @dependabot[bot] in https://github.com/mudler/LocalAI/pull/5878
* chore: :arrow_up: Update richiejp/stable-diffusion.cpp to `10c6501bd05a697e014f1bee3a84e5664290c489` by @localai-bot in https://github.com/mudler/LocalAI/pull/5732
* fix(stablediffusion-cpp): Switch back to upstream and update by @richiejp in https://github.com/mudler/LocalAI/pull/5880
### Other Changes
* docs: :arrow_up: update docs version mudler/LocalAI by @localai-bot in https://github.com/mudler/LocalAI/pull/5752
* docs: :arrow_up: update docs version mudler/LocalAI by @localai-bot in https://github.com/mudler/LocalAI/pull/5775
* docs: :arrow_up: update docs version mudler/LocalAI by @localai-bot in https://github.com/mudler/LocalAI/pull/5781
* chore: :arrow_up: Update ggml-org/llama.cpp to `bf9087f59aab940cf312b85a67067ce33d9e365a` by @localai-bot in https://github.com/mudler/LocalAI/pull/5860
* chore: :arrow_up: Update ggml-org/llama.cpp to `a979ca22db0d737af1e548a73291193655c6be99` by @localai-bot in https://github.com/mudler/LocalAI/pull/5862
* chore: :arrow_up: Update ggml-org/llama.cpp to `2be60cbc2707359241c2784f9d2e30d8fc7cdabb` by @localai-bot in https://github.com/mudler/LocalAI/pull/5867
* chore: :arrow_up: Update ggml-org/whisper.cpp to `1f5cf0b2888402d57bb17b2029b2caa97e5f3baf` by @localai-bot in https://github.com/mudler/LocalAI/pull/5876
* chore: :arrow_up: Update ggml-org/llama.cpp to `6c9ee3b17e19dcc82ab93d52ae46fdd0226d4777` by @localai-bot in https://github.com/mudler/LocalAI/pull/5877
* chore: drop vllm for cuda 11 by @mudler in https://github.com/mudler/LocalAI/pull/5881
* chore: :arrow_up: Update ggml-org/llama.cpp to `acd6cb1c41676f6bbb25c2a76fa5abeb1719301e` by @localai-bot in https://github.com/mudler/LocalAI/pull/5882
* fix: rename Dockerfile.go --&gt; Dockerfile.golang to avoid IDE errors by @dave-gray101 in https://github.com/mudler/LocalAI/pull/5892
* chore(Makefile): drop unused targets by @mudler in https://github.com/mudler/LocalAI/pull/5893
* chore: :arrow_up: Update ggml-org/llama.cpp to `a86f52b2859dae4db5a7a0bbc0f1ad9de6b43ec6` by @localai-bot in https://github.com/mudler/LocalAI/pull/5894
* fix: untangle pkg and core by @dave-gray101 in https://github.com/mudler/LocalAI/pull/5896
* Update quickstart.md by @Shinrai in https://github.com/mudler/LocalAI/pull/5898

&lt;/details&gt;

&lt;h2&gt;New Contributors&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;@dedyf5 made their first contribution in https://github.com/mudler/LocalAI/pull/5770&lt;/li&gt;
&lt;li&gt;@Shinrai made their first contribution in https://github.com/mudler/LocalAI/pull/5898&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Full Changelog&lt;/strong&gt;: https://github.com/mudler/LocalAI/compare/v3.1.1...v3.2.0&lt;/p&gt;</content>
    <link href="https://github.com/mudler/LocalAI/releases/tag/v3.2.0" rel="alternate"/>
  </entry>
  <entry>
    <id>https://github.com/mudler/LocalAI/releases/tag/v3.2.1</id>
    <title>New release for LocalAI: v3.2.1</title>
    <updated>2025-07-25T10:39:36-04:00</updated>
    <author>
      <name>mudler/LocalAI</name>
    </author>
    <content>&lt;!-- Release notes generated using configuration in .github/release.yml at v3.2.1 --&gt;

&lt;h2&gt;What's Changed&lt;/h2&gt;
&lt;h3&gt;Bug fixes :bug:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;fix(install.sh): update to use the new binary naming by @mudler in https://github.com/mudler/LocalAI/pull/5903&lt;/li&gt;
&lt;li&gt;fix(backends gallery): pass-by backend galleries to the model service by @mudler in https://github.com/mudler/LocalAI/pull/5906&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Other Changes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;chore: :arrow_up: Update ggml-org/llama.cpp to &lt;code&gt;3f4fc97f1d745f1d5d3c853949503136d419e6de&lt;/code&gt; by @localai-bot in https://github.com/mudler/LocalAI/pull/5900&lt;/li&gt;
&lt;li&gt;chore: :arrow_up: Update leejet/stable-diffusion.cpp to &lt;code&gt;eed97a5e1d054f9c1e7ac01982ae480411d4157e&lt;/code&gt; by @localai-bot in https://github.com/mudler/LocalAI/pull/5901&lt;/li&gt;
&lt;li&gt;chore: :arrow_up: Update ggml-org/whisper.cpp to &lt;code&gt;7de8dd783f7b2eab56bff6bbc5d3369e34f0e77f&lt;/code&gt; by @localai-bot in https://github.com/mudler/LocalAI/pull/5902&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Full Changelog&lt;/strong&gt;: https://github.com/mudler/LocalAI/compare/v3.2.0...v3.2.1&lt;/p&gt;</content>
    <link href="https://github.com/mudler/LocalAI/releases/tag/v3.2.1" rel="alternate"/>
  </entry>
  <entry>
    <id>https://github.com/mudler/LocalAI/releases/tag/v3.2.2</id>
    <title>New release for LocalAI: v3.2.2</title>
    <updated>2025-07-25T17:05:04-04:00</updated>
    <author>
      <name>mudler/LocalAI</name>
    </author>
    <content>&lt;!-- Release notes generated using configuration in .github/release.yml at master --&gt;

&lt;h2&gt;What's Changed&lt;/h2&gt;
&lt;h3&gt;Bug fixes :bug:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;fix(backends gallery): trim string when reading cap from file by @mudler in https://github.com/mudler/LocalAI/pull/5909&lt;/li&gt;
&lt;li&gt;fix(vulkan): use correct image suffix by @mudler in https://github.com/mudler/LocalAI/pull/5911&lt;/li&gt;
&lt;li&gt;fix(ci): add nvidia-l4t capability to l4t images by @mudler in https://github.com/mudler/LocalAI/pull/5914&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Exciting New Features üéâ&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;feat(backend gallery): add mirrors by @mudler in https://github.com/mudler/LocalAI/pull/5910&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Other Changes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;docs: :arrow_up: update docs version mudler/LocalAI by @localai-bot in https://github.com/mudler/LocalAI/pull/5912&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Full Changelog&lt;/strong&gt;: https://github.com/mudler/LocalAI/compare/v3.2.1...v3.2.2&lt;/p&gt;</content>
    <link href="https://github.com/mudler/LocalAI/releases/tag/v3.2.2" rel="alternate"/>
  </entry>
  <entry>
    <id>https://github.com/mudler/LocalAI/releases/tag/v3.2.3</id>
    <title>New release for LocalAI: v3.2.3</title>
    <updated>2025-07-26T02:31:22-04:00</updated>
    <author>
      <name>mudler/LocalAI</name>
    </author>
    <content>&lt;!-- Release notes generated using configuration in .github/release.yml at master --&gt;

&lt;h2&gt;What's Changed&lt;/h2&gt;
&lt;h3&gt;Bug fixes :bug:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;fix(cuda): be consistent with image tag naming by @mudler in https://github.com/mudler/LocalAI/pull/5916&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;üìñ Documentation and examples&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;chore(docs): add documentation on backend detection override by @mudler in https://github.com/mudler/LocalAI/pull/5915&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Other Changes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;chore: :arrow_up: Update ggml-org/llama.cpp to &lt;code&gt;c7f3169cd523140a288095f2d79befb20a0b73f4&lt;/code&gt; by @localai-bot in https://github.com/mudler/LocalAI/pull/5913&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Full Changelog&lt;/strong&gt;: https://github.com/mudler/LocalAI/compare/v3.2.2...v3.2.3&lt;/p&gt;</content>
    <link href="https://github.com/mudler/LocalAI/releases/tag/v3.2.3" rel="alternate"/>
  </entry>
  <entry>
    <id>https://github.com/mudler/LocalAI/releases/tag/v3.3.0</id>
    <title>New release for LocalAI: v3.3.0</title>
    <updated>2025-07-28T11:03:28-04:00</updated>
    <author>
      <name>mudler/LocalAI</name>
    </author>
    <content>&lt;h1 align="center"&gt;
  &lt;br&gt;
  &lt;img height="300" src="https://raw.githubusercontent.com/mudler/LocalAI/refs/heads/master/core/http/static/logo.png"&gt; &lt;br&gt;
&lt;br&gt;
üöÄ LocalAI 3.3.0
&lt;/h1&gt;

&lt;h2&gt;What‚Äôs New in LocalAI 3.3.0 üéâ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Object detection! From 3.3.0, now LocalAI supports with a new API - also fast object detection! Just install the &lt;code&gt;rfdetr-base&lt;/code&gt; model - See &lt;a href="https://localai.io/features/object-detection/"&gt;the documentation&lt;/a&gt; to learn more&lt;/li&gt;
&lt;li&gt;Backends now have defined mirrors for download - this helps when primary registries fails during download&lt;/li&gt;
&lt;li&gt;Bug fixes: worked hard into squashing bugfixes in this release! Ranging from container images to backends and installation scripts&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;The Complete Local Stack for Privacy-First AI&lt;/h2&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td width="30%" valign="top" align="center"&gt;
      &lt;a href="https://github.com/mudler/LocalAI"&gt;
        &lt;img src="https://raw.githubusercontent.com/mudler/LocalAI/refs/heads/master/core/http/static/logo.png" width="200" alt="LocalAI Logo"&gt;
        &lt;h3&gt;LocalAI&lt;/h3&gt;
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td width="70%" valign="top"&gt;
      &lt;p&gt;The free, Open Source OpenAI alternative. Acts as a drop-in replacement REST API compatible with OpenAI specifications for local AI inferencing. No GPU required.&lt;/p&gt;
      &lt;p&gt;&lt;em&gt;Link:&lt;/em&gt; &lt;a href="https://github.com/mudler/LocalAI"&gt;https://github.com/mudler/LocalAI&lt;/a&gt;&lt;/p&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width="30%" valign="top" align="center"&gt;
      &lt;a href="https://github.com/mudler/LocalAGI"&gt;
         &lt;img src="https://raw.githubusercontent.com/mudler/LocalAGI/refs/heads/main/webui/react-ui/public/logo_2.png" width="200" alt="LocalAGI Logo"&gt;
         &lt;h3&gt;LocalAGI&lt;/h3&gt;
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td width="70%" valign="top"&gt;
      &lt;p&gt;A powerful Local AI agent management platform. Serves as a drop-in replacement for OpenAI's Responses API, supercharged with advanced agentic capabilities and a no-code UI.&lt;/p&gt;
      &lt;p&gt;&lt;em&gt;Link:&lt;/em&gt; &lt;a href="https://github.com/mudler/LocalAGI"&gt;https://github.com/mudler/LocalAGI&lt;/a&gt;&lt;/p&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width="30%" valign="top" align="center"&gt;
      &lt;a href="https://github.com/mudler/LocalRecall"&gt;
         &lt;img src="https://raw.githubusercontent.com/mudler/LocalRecall/refs/heads/main/static/localrecall_horizontal.png" width="200" alt="LocalRecall Logo"&gt;
         &lt;h3&gt;LocalRecall&lt;/h3&gt;
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td width="70%" valign="top"&gt;
      &lt;p&gt;A RESTful API and knowledge base management system providing persistent memory and storage capabilities for AI agents. Designed to work alongside LocalAI and LocalAGI.&lt;/p&gt;
      &lt;p&gt;&lt;em&gt;Link:&lt;/em&gt; &lt;a href="https://github.com/mudler/LocalRecall"&gt;https://github.com/mudler/LocalRecall&lt;/a&gt;&lt;/p&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h2&gt;Thank you! ‚ù§Ô∏è&lt;/h2&gt;
&lt;p&gt;A massive &lt;strong&gt;THANK YOU&lt;/strong&gt; to our incredible community and our sponsors! LocalAI has over &lt;strong&gt;34,100 stars&lt;/strong&gt;, and LocalAGI has already rocketed past &lt;strong&gt;900+ stars&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;As a reminder, LocalAI is real FOSS (Free and Open Source Software) and its sibling projects are community-driven and not backed by VCs or a company. We rely on contributors donating their spare time and our sponsors to provide us the hardware! If you love open-source, privacy-first AI, please consider starring the repos, contributing code, reporting bugs, or spreading the word!&lt;/p&gt;
&lt;p&gt;üëâ &lt;strong&gt;Check out the reborn LocalAGI v2 today:&lt;/strong&gt; &lt;a href="https://github.com/mudler/LocalAGI"&gt;https://github.com/mudler/LocalAGI&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Full changelog :point_down:&lt;/h2&gt;
&lt;details&gt;

&lt;summary&gt;
:point_right: Click to expand :point_left: 
&lt;/summary&gt;

## What's Changed
### Bug fixes :bug:
* fix(backend gallery): intel images for python-based backends, re-add exllama2 by @mudler in https://github.com/mudler/LocalAI/pull/5928
### Exciting New Features üéâ
* feat: normalize search by @mudler in https://github.com/mudler/LocalAI/pull/5925
* feat(rfdetr): add object detection API by @mudler in https://github.com/mudler/LocalAI/pull/5923
### Other Changes
* docs: :arrow_up: update docs version mudler/LocalAI by @localai-bot in https://github.com/mudler/LocalAI/pull/5920
* chore: :arrow_up: Update ggml-org/whisper.cpp to `e7bf0294ec9099b5fc21f5ba969805dfb2108cea` by @localai-bot in https://github.com/mudler/LocalAI/pull/5922
* chore: :arrow_up: Update ggml-org/llama.cpp to `11dd5a44eb180e1d69fac24d3852b5222d66fb7f` by @localai-bot in https://github.com/mudler/LocalAI/pull/5921
* chore: drop assistants endpoint by @mudler in https://github.com/mudler/LocalAI/pull/5926
* chore: :arrow_up: Update ggml-org/llama.cpp to `bf78f5439ee8e82e367674043303ebf8e92b4805` by @localai-bot in https://github.com/mudler/LocalAI/pull/5927

&lt;/details&gt;

&lt;p&gt;&lt;strong&gt;Full Changelog&lt;/strong&gt;: https://github.com/mudler/LocalAI/compare/v3.2.3...v3.3.0&lt;/p&gt;</content>
    <link href="https://github.com/mudler/LocalAI/releases/tag/v3.3.0" rel="alternate"/>
  </entry>
</feed>
