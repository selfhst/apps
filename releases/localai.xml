<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <id>https://selfh.st/apps</id>
  <title>LocalAI Releases</title>
  <updated>2025-07-25T16:36:35.213873-04:00</updated>
  <author>
    <name>selfh.st</name>
    <email>contact@selfh.st</email>
  </author>
  <link href="https://selfh.st/apps" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="0.9.0">python-feedgen</generator>
  <subtitle>Self-hosted software releases generated by selfh.st</subtitle>
  <entry>
    <id>https://github.com/mudler/LocalAI/releases/tag/v3.0.0</id>
    <title>New release for LocalAI: v3.0.0</title>
    <updated>2025-06-19T11:55:10-04:00</updated>
    <author>
      <name>mudler/LocalAI</name>
    </author>
    <content>&lt;h1 align="center"&gt;
  &lt;br&gt;
  &lt;img height="300" src="https://raw.githubusercontent.com/mudler/LocalAI/refs/heads/master/core/http/static/logo.png"&gt; &lt;br&gt;
&lt;br&gt;
üöÄ LocalAI 3.0 ‚Äì A New Era Begins
&lt;/h1&gt;

&lt;p&gt;Say hello to LocalAI 3.0 ‚Äî our most ambitious release yet!&lt;/p&gt;
&lt;p&gt;We‚Äôve taken huge strides toward making LocalAI not just local, but limitless. Whether you're building LLM-powered agents, experimenting with audio pipelines, or deploying multimodal backends at scale ‚Äî this release is for you.&lt;/p&gt;
&lt;p&gt;Let‚Äôs walk you through what‚Äôs new. (And yes, there‚Äôs a lot to love.)&lt;/p&gt;
&lt;h2&gt;TL;DR ‚Äì What‚Äôs New in LocalAI 3.0.0 üéâ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;üß© Backend Gallery: Install/remove backends on the fly, powered by OCI images ‚Äî fully customizable and API-driven.&lt;/li&gt;
&lt;li&gt;üéôÔ∏è Audio Support: Upload audio, PDFs, or text in the UI ‚Äî plus new audio understanding models like Qwen Omni.&lt;/li&gt;
&lt;li&gt;üåê Realtime API: WebSocket support compatible with OpenAI clients, great for chat apps and agents.&lt;/li&gt;
&lt;li&gt;üß† Reasoning UI Boosts: Thinking indicators now show in chat for smart models.&lt;/li&gt;
&lt;li&gt;üìä Dynamic VRAM Handling: Smarter GPU usage with automatic offloading.&lt;/li&gt;
&lt;li&gt;ü¶ô Llama.cpp Upgrades: Now with reranking + multimodal via libmtmd.&lt;/li&gt;
&lt;li&gt;üì¶ 50+ New Models: Huge model gallery update with fresh LLMs across categories.&lt;/li&gt;
&lt;li&gt;üêû Bug Fixes: Streamed runes, template stability, better backend gallery UX.&lt;/li&gt;
&lt;li&gt;‚ùå Deprecated: Extras images ‚Äî replaced by the new backend system.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;üëâ Dive into the full changelog and docs below to explore more!&lt;/p&gt;
&lt;h2&gt;üß© Introducing the Backend Gallery ‚Äî Plug, Play, Power Up&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Screenshot From 2025-06-19 16-48-58" src="https://github.com/user-attachments/assets/2166d107-b1cf-4cb3-ad39-e2edc80338a8" /&gt;&lt;/p&gt;
&lt;p&gt;No more hunting for dependencies or custom hacks.&lt;/p&gt;
&lt;p&gt;With the new Backend Gallery, you can now:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Install &amp;amp; remove backends at runtime or startup via API or directly from the WebUI&lt;/li&gt;
&lt;li&gt;Use custom galleries, just like you do for models&lt;/li&gt;
&lt;li&gt;Enjoy zero-config access to the default LocalAI gallery&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Backends are standard OCI images ‚Äî portable, composable, and totally DIY-friendly. Goodbye to "extras images" ‚Äî hello to full backend modularity, even with Python-based dependencies.&lt;/p&gt;
&lt;p&gt;üìñ &lt;a href="https://localai.io/backends"&gt;Explore the Backend Gallery Docs&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;‚ö†Ô∏è Important: Breaking Changes&lt;/h3&gt;
&lt;p&gt;From this release we will stop pushing &lt;code&gt;-extra&lt;/code&gt; images containing python backends. You can now use standard images, and you will have only to pick the ones that are suited for your GPU. Additional backends can be installed via the backend gallery.&lt;/p&gt;
&lt;p&gt;Here below some examples, note that the CI is still publishing the images so won't be available until jobs are processed, and the installation scripts will be updated right after images are publicly available.&lt;/p&gt;
&lt;h3&gt;CPU only image:&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;bash
docker run -ti --name local-ai -p 8080:8080 localai/localai:latest&lt;/code&gt;&lt;/p&gt;
&lt;h3&gt;NVIDIA GPU Images:&lt;/h3&gt;
&lt;p&gt;```bash&lt;/p&gt;
&lt;h1&gt;CUDA 12&lt;/h1&gt;
&lt;p&gt;docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-gpu-nvidia-cuda-12&lt;/p&gt;
&lt;h1&gt;CUDA 11&lt;/h1&gt;
&lt;p&gt;docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-gpu-nvidia-cuda-11&lt;/p&gt;
&lt;h1&gt;NVIDIA Jetson (L4T) ARM64&lt;/h1&gt;
&lt;p&gt;docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-nvidia-l4t-arm64
```&lt;/p&gt;
&lt;h3&gt;AMD GPU Images (ROCm):&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;bash
docker run -ti --name local-ai -p 8080:8080 --device=/dev/kfd --device=/dev/dri --group-add=video localai/localai:latest-gpu-hipblas&lt;/code&gt;&lt;/p&gt;
&lt;h3&gt;Intel GPU Images (oneAPI):&lt;/h3&gt;
&lt;p&gt;```bash&lt;/p&gt;
&lt;h1&gt;Intel GPU with FP16 support&lt;/h1&gt;
&lt;p&gt;docker run -ti --name local-ai -p 8080:8080 localai/localai:latest-gpu-intel-f16&lt;/p&gt;
&lt;h1&gt;Intel GPU with FP32 support&lt;/h1&gt;
&lt;p&gt;docker run -ti --name local-ai -p 8080:8080 localai/localai:latest-gpu-intel-f32
```&lt;/p&gt;
&lt;h3&gt;Vulkan GPU Images:&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;bash
docker run -ti --name local-ai -p 8080:8080 localai/localai:latest-gpu-vulkan&lt;/code&gt;&lt;/p&gt;
&lt;h3&gt;AIO Images (pre-downloaded models):&lt;/h3&gt;
&lt;p&gt;```bash&lt;/p&gt;
&lt;h1&gt;CPU version&lt;/h1&gt;
&lt;p&gt;docker run -ti --name local-ai -p 8080:8080 localai/localai:latest-aio-cpu&lt;/p&gt;
&lt;h1&gt;NVIDIA CUDA 12 version&lt;/h1&gt;
&lt;p&gt;docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-aio-gpu-nvidia-cuda-12&lt;/p&gt;
&lt;h1&gt;NVIDIA CUDA 11 version&lt;/h1&gt;
&lt;p&gt;docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-aio-gpu-nvidia-cuda-11&lt;/p&gt;
&lt;h1&gt;Intel GPU version&lt;/h1&gt;
&lt;p&gt;docker run -ti --name local-ai -p 8080:8080 localai/localai:latest-aio-gpu-intel-f16&lt;/p&gt;
&lt;h1&gt;AMD GPU version&lt;/h1&gt;
&lt;p&gt;docker run -ti --name local-ai -p 8080:8080 --device=/dev/kfd --device=/dev/dri --group-add=video localai/localai:latest-aio-gpu-hipblas
```&lt;/p&gt;
&lt;p&gt;For more information about the AIO images and pre-downloaded models, see &lt;a href="https://localai.io/basics/container/"&gt;Container Documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;üß† Smarter Reasoning, Smoother Chat&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Screenshot From 2025-06-19 16-58-27" src="https://github.com/user-attachments/assets/86a76557-eab0-4323-b928-2579692ac276" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Realtime WebSocket API: OpenAI-style streaming support via WebSocket is here. Ideal for agents and chat apps.&lt;/li&gt;
&lt;li&gt;"Thinking" Tags: Reasoning models now show a visual "thinking" box during inference in the UI. Intuitive and satisfying.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;üß† Model Power-Up: VRAM Savvy + Multimodal Brains&lt;/h2&gt;
&lt;p&gt;Dynamic VRAM Estimation: LocalAI now adapts and offloads layers depending on your GPU‚Äôs capabilities. Optimal performance, no guesswork.
Llama.cpp upgrades also includes:
 - reranking
 - Enhanced multimodal support via libmtmd&lt;/p&gt;
&lt;h2&gt;üß™ New Models!&lt;/h2&gt;
&lt;p&gt;More than 50 new models joined the gallery, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;üß† skywork-or1-32b, rivermind-lux-12b, qwen3-embedding-*, llama3-24b-mullein, ultravox-v0_5, and more&lt;/li&gt;
&lt;li&gt;üß¨ Multimodal, reasoning, and domain-specific LLMs for every need&lt;/li&gt;
&lt;li&gt;üì¶ Browse the latest additions in the &lt;a href="https://models.localai.io"&gt;Model Gallery&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;üêû Bugfixes &amp;amp; Polish&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Rune streaming is now buttery smooth&lt;/li&gt;
&lt;li&gt;Countless fixes across templates, inputs, CI, and realtime session updates&lt;/li&gt;
&lt;li&gt;Backend gallery UI is more stable and informative&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;The Complete Local Stack for Privacy-First AI&lt;/h2&gt;
&lt;p&gt;With LocalAGI rejoining LocalAI alongside LocalRecall, our ecosystem provides a complete, open-source stack for private, secure, and intelligent AI operations:&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td width="30%" valign="top" align="center"&gt;
      &lt;a href="https://github.com/mudler/LocalAI"&gt;
        &lt;img src="https://raw.githubusercontent.com/mudler/LocalAI/refs/heads/master/core/http/static/logo.png" width="200" alt="LocalAI Logo"&gt;
        &lt;h3&gt;LocalAI&lt;/h3&gt;
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td width="70%" valign="top"&gt;
      &lt;p&gt;The free, Open Source OpenAI alternative. Acts as a drop-in replacement REST API compatible with OpenAI specifications for local AI inferencing. No GPU required.&lt;/p&gt;
      &lt;p&gt;&lt;em&gt;Link:&lt;/em&gt; &lt;a href="https://github.com/mudler/LocalAI"&gt;https://github.com/mudler/LocalAI&lt;/a&gt;&lt;/p&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width="30%" valign="top" align="center"&gt;
      &lt;a href="https://github.com/mudler/LocalAGI"&gt;
         &lt;img src="https://raw.githubusercontent.com/mudler/LocalAGI/refs/heads/main/webui/react-ui/public/logo_2.png" width="200" alt="LocalAGI Logo"&gt;
         &lt;h3&gt;LocalAGI&lt;/h3&gt;
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td width="70%" valign="top"&gt;
      &lt;p&gt;A powerful Local AI agent management platform. Serves as a drop-in replacement for OpenAI's Responses API, supercharged with advanced agentic capabilities and a no-code UI.&lt;/p&gt;
      &lt;p&gt;&lt;em&gt;Link:&lt;/em&gt; &lt;a href="https://github.com/mudler/LocalAGI"&gt;https://github.com/mudler/LocalAGI&lt;/a&gt;&lt;/p&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width="30%" valign="top" align="center"&gt;
      &lt;a href="https://github.com/mudler/LocalRecall"&gt;
         &lt;img src="https://raw.githubusercontent.com/mudler/LocalRecall/refs/heads/main/static/localrecall_horizontal.png" width="200" alt="LocalRecall Logo"&gt;
         &lt;h3&gt;LocalRecall&lt;/h3&gt;
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td width="70%" valign="top"&gt;
      &lt;p&gt;A RESTful API and knowledge base management system providing persistent memory and storage capabilities for AI agents. Designed to work alongside LocalAI and LocalAGI.&lt;/p&gt;
      &lt;p&gt;&lt;em&gt;Link:&lt;/em&gt; &lt;a href="https://github.com/mudler/LocalRecall"&gt;https://github.com/mudler/LocalRecall&lt;/a&gt;&lt;/p&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h2&gt;Join the Movement! ‚ù§Ô∏è&lt;/h2&gt;
&lt;p&gt;A massive &lt;strong&gt;THANK YOU&lt;/strong&gt; to our incredible community and our sponsors! LocalAI has over &lt;strong&gt;33,300 stars&lt;/strong&gt;, and LocalAGI has already rocketed past &lt;strong&gt;750+ stars&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;As a reminder, LocalAI is real FOSS (Free and Open Source Software) and its sibling projects are community-driven and not backed by VCs or a company. We rely on contributors donating their spare time and our sponsors to provide us the hardware! If you love open-source, privacy-first AI, please consider starring the repos, contributing code, reporting bugs, or spreading the word!&lt;/p&gt;
&lt;p&gt;üëâ &lt;strong&gt;Check out the reborn LocalAGI v2 today:&lt;/strong&gt; &lt;a href="https://github.com/mudler/LocalAGI"&gt;https://github.com/mudler/LocalAGI&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;LocalAI 3.0.0 is here. What will you build next?&lt;/p&gt;
&lt;h2&gt;Full changelog :point_down:&lt;/h2&gt;
&lt;details&gt;

&lt;summary&gt;
:point_right: Click to expand :point_left: 
&lt;/summary&gt;


&lt;!-- Release notes generated using configuration in .github/release.yml at master --&gt;

## What's Changed
### Breaking Changes üõ†
* feat: Add backend gallery by @mudler in https://github.com/mudler/LocalAI/pull/5607
* chore(backends): move `bark-cpp` to the backend gallery by @mudler in https://github.com/mudler/LocalAI/pull/5682
### Bug fixes :bug:
* fix(ci): tag latest against cpu-only image by @mudler in https://github.com/mudler/LocalAI/pull/5362
* fix(flux): Set CFG=1 so that prompts are followed by @richiejp in https://github.com/mudler/LocalAI/pull/5378
* fix(template): we do not always have .Name by @mudler in https://github.com/mudler/LocalAI/pull/5508
* fix(input): handle correctly case where we pass by string list as inputs by @mudler in https://github.com/mudler/LocalAI/pull/5521
* fix(streaming): stream complete runes by @mudler in https://github.com/mudler/LocalAI/pull/5539
* fix(install.sh): vulkan docker tag by @halkeye in https://github.com/mudler/LocalAI/pull/5589
* fix(realtime): Use updated model on session update by @richiejp in https://github.com/mudler/LocalAI/pull/5604
* fix(backends gallery): propagate p2p settings to correctly draw menu by @mudler in https://github.com/mudler/LocalAI/pull/5684
### Exciting New Features üéâ
* feat(llama.cpp): upgrade and use libmtmd by @mudler in https://github.com/mudler/LocalAI/pull/5379
* feat(ui): add error page to display errors by @mudler in https://github.com/mudler/LocalAI/pull/5418
* feat(llama.cpp): add reranking by @mudler in https://github.com/mudler/LocalAI/pull/5396
* feat: Realtime API support reboot by @richiejp in https://github.com/mudler/LocalAI/pull/5392
* feat(llama.cpp): add support for audio input by @mudler in https://github.com/mudler/LocalAI/pull/5466
* feat(ui): add audio upload button in chat view by @mudler in https://github.com/mudler/LocalAI/pull/5526
* feat(ui): allow to upload PDF and text files, also add support to multiple input files by @mudler in https://github.com/mudler/LocalAI/pull/5538
* feat(ui): display thinking tags appropriately by @mudler in https://github.com/mudler/LocalAI/pull/5540
* feat: improve RAM estimation by using values from summary by @mudler in https://github.com/mudler/LocalAI/pull/5525
* feat(backend gallery): display download progress by @mudler in https://github.com/mudler/LocalAI/pull/5687
### üß† Models
* chore(model gallery): add skywork_skywork-or1-32b by @mudler in https://github.com/mudler/LocalAI/pull/5369
* chore(model gallery): add skywork_skywork-or1-7b by @mudler in https://github.com/mudler/LocalAI/pull/5370
* chore(model gallery): add thedrummer_snowpiercer-15b-v1 by @mudler in https://github.com/mudler/LocalAI/pull/5371
* chore(model gallery): add thedrummer_rivermind-lux-12b-v1 by @mudler in https://github.com/mudler/LocalAI/pull/5372
* chore(model gallery): add primeintellect_intellect-2 by @mudler in https://github.com/mudler/LocalAI/pull/5373
* fix: typos by @omahs in https://github.com/mudler/LocalAI/pull/5376
* chore(model gallery): add soob3123_grayline-qwen3-14b by @mudler in https://github.com/mudler/LocalAI/pull/5393
* chore(model gallery): add soob3123_grayline-qwen3-8b by @mudler in https://github.com/mudler/LocalAI/pull/5394
* chore(model gallery): add a-m-team_am-thinking-v1 by @mudler in https://github.com/mudler/LocalAI/pull/5395
* chore(model gallery): add thedrummer_valkyrie-49b-v1 by @mudler in https://github.com/mudler/LocalAI/pull/5410
* chore(model gallery): add facebook_kernelllm by @mudler in https://github.com/mudler/LocalAI/pull/5411
* chore(model gallery): add smolvlm-256m-instruct by @mudler in https://github.com/mudler/LocalAI/pull/5412
* chore(model gallery): add smolvlm-500m-instruct by @mudler in https://github.com/mudler/LocalAI/pull/5413
* chore(model gallery): add smolvlm-instruct by @mudler in https://github.com/mudler/LocalAI/pull/5414
* chore(model gallery): add smolvlm2-2.2b-instruct by @mudler in https://github.com/mudler/LocalAI/pull/5415
* chore(model gallery): add smolvlm2-500m-video-instruct by @mudler in https://github.com/mudler/LocalAI/pull/5416
* chore(model gallery): add smolvlm2-256m-video-instruct by @mudler in https://github.com/mudler/LocalAI/pull/5417
* chore(model-gallery): :arrow_up: update checksum by @localai-bot in https://github.com/mudler/LocalAI/pull/5422
* chore(model gallery): add nvidia_llama-3.1-nemotron-nano-4b-v1.1 by @mudler in https://github.com/mudler/LocalAI/pull/5427
* chore(model gallery): add mistralai_devstral-small-2505 by @mudler in https://github.com/mudler/LocalAI/pull/5428
* chore(model gallery): add delta-vector_archaeo-12b-v2 by @mudler in https://github.com/mudler/LocalAI/pull/5429
* chore(model gallery): add arliai_qwq-32b-arliai-rpr-v4 by @mudler in https://github.com/mudler/LocalAI/pull/5443
* chore(model gallery): add whiterabbitneo_whiterabbitneo-v3-7b by @mudler in https://github.com/mudler/LocalAI/pull/5444
* chore(model gallery): add vulpecula-4b by @mudler in https://github.com/mudler/LocalAI/pull/5445
* chore(model gallery): add medgemma-4b-it by @mudler in https://github.com/mudler/LocalAI/pull/5460
* chore(model gallery): add medgemma-27b-text-it by @mudler in https://github.com/mudler/LocalAI/pull/5461
* chore(model gallery): add allura-org_q3-30b-a3b-pentiment by @mudler in https://github.com/mudler/LocalAI/pull/5462
* chore(model gallery): add allura-org_q3-30b-a3b-designant by @mudler in https://github.com/mudler/LocalAI/pull/5502
* chore(model gallery): add luckyrp-24b by @mudler in https://github.com/mudler/LocalAI/pull/5503
* chore(model gallery): add mrm8488_qwen3-14b-ft-limo by @mudler in https://github.com/mudler/LocalAI/pull/5504
* chore(model gallery): add llama3-24b-mullein-v1 by @mudler in https://github.com/mudler/LocalAI/pull/5505
* chore(model gallery): add ms-24b-mullein-v0 by @mudler in https://github.com/mudler/LocalAI/pull/5506
* chore(model gallery): add qwen2.5-omni-7b by @mudler in https://github.com/mudler/LocalAI/pull/5513
* chore(model gallery): add pku-ds-lab_fairyr1-14b-preview by @mudler in https://github.com/mudler/LocalAI/pull/5516
* chore(model gallery): add pku-ds-lab_fairyr1-32b by @mudler in https://github.com/mudler/LocalAI/pull/5517
* chore(model gallery): add moondream2-20250414 by @mudler in https://github.com/mudler/LocalAI/pull/5518
* chore(model gallery): add arcee-ai_homunculus by @mudler in https://github.com/mudler/LocalAI/pull/5577
* chore(model gallery): add nvidia_nemotron-research-reasoning-qwen-1.5b by @mudler in https://github.com/mudler/LocalAI/pull/5578
* chore(model gallery): add e-n-v-y_legion-v2.1-llama-70b-elarablated-v0.8-hf by @mudler in https://github.com/mudler/LocalAI/pull/5579
* chore(model gallery): add deepseek-ai_deepseek-r1-0528-qwen3-8b by @mudler in https://github.com/mudler/LocalAI/pull/5580
* chore(model gallery): add goekdeniz-guelmez_josiefied-qwen3-14b-abliterated-v3 by @mudler in https://github.com/mudler/LocalAI/pull/5590
* chore(model gallery): add ultravox-v0_5-llama-3_2-1b by @mudler in https://github.com/mudler/LocalAI/pull/5591
* chore(model gallery): add ultravox-v0_5-llama-3_1-8b by @mudler in https://github.com/mudler/LocalAI/pull/5592
* chore(model gallery): add open-thoughts_openthinker3-7b by @mudler in https://github.com/mudler/LocalAI/pull/5595
* chore(model gallery): add nbeerbower_qwen3-gutenberg-encore-14b by @mudler in https://github.com/mudler/LocalAI/pull/5596
* chore(model gallery): add akhil-theerthala_kuvera-8b-v0.1.0 by @mudler in https://github.com/mudler/LocalAI/pull/5600
* chore(model gallery): add qwen2.5-omni-3b by @mudler in https://github.com/mudler/LocalAI/pull/5606
* chore(model gallery): add kwaipilot_kwaicoder-autothink-preview by @mudler in https://github.com/mudler/LocalAI/pull/5627
* chore(model gallery): add sophosympatheia_strawberrylemonade-l3-70b-v1.0 by @mudler in https://github.com/mudler/LocalAI/pull/5628
* chore(model gallery): add mistralai_magistral-small-2506 by @mudler in https://github.com/mudler/LocalAI/pull/5629
* chore(model gallery): add baai_robobrain2.0-7b by @mudler in https://github.com/mudler/LocalAI/pull/5630
* chore(model gallery): add openbuddy_openbuddy-r1-0528-distill-qwen3-32b-preview0-qat by @mudler in https://github.com/mudler/LocalAI/pull/5631
* chore(model gallery): add qwen3-embedding-4b by @mudler in https://github.com/mudler/LocalAI/pull/5632
* chore(model gallery): add qwen3-embedding-8b by @mudler in https://github.com/mudler/LocalAI/pull/5633
* chore(model gallery): add qwen3-embedding-0.6b by @mudler in https://github.com/mudler/LocalAI/pull/5634
* chore(model gallery): add yanfei-v2-qwen3-32b by @mudler in https://github.com/mudler/LocalAI/pull/5639
### üìñ Documentation and examples
* chore(docs/install.sh): image changes by @mudler in https://github.com/mudler/LocalAI/pull/5354
* updating the documentation on fine tuning and advanced guide.  by @TheDarkTrumpet in https://github.com/mudler/LocalAI/pull/5420
### üëí Dependencies
* chore: :arrow_up: Update ggml-org/whisper.cpp to `e41bc5c61ae66af6be2bd7011769bb821a83e8ae` by @localai-bot in https://github.com/mudler/LocalAI/pull/5357
* chore: :arrow_up: Update ggml-org/llama.cpp to `de4c07f93783a1a96456a44dc16b9db538ee1618` by @localai-bot in https://github.com/mudler/LocalAI/pull/5358
* chore: :arrow_up: Update ggml-org/whisper.cpp to `f89056057511a1657af90bb28ef3f21e5b1f33cd` by @localai-bot in https://github.com/mudler/LocalAI/pull/5364
* chore: :arrow_up: Update ggml-org/whisper.cpp to `f389d7e3e56bbbfec49fd333551927a0fcbb7213` by @localai-bot in https://github.com/mudler/LocalAI/pull/5367
* chore: :arrow_up: Update ggml-org/whisper.cpp to `20a20decd94badfd519a07ea91f0bba8b8fc4dea` by @localai-bot in https://github.com/mudler/LocalAI/pull/5374
* chore: :arrow_up: Update ggml-org/whisper.cpp to `d1f114da61b1ae1e70b03104fad42c9dd666feeb` by @localai-bot in https://github.com/mudler/LocalAI/pull/5381
* chore: :arrow_up: Update ggml-org/llama.cpp to `e3a7cf6c5bf6a0a24217f88607b06e4405a2b5d9` by @localai-bot in https://github.com/mudler/LocalAI/pull/5384
* chore: :arrow_up: Update ggml-org/llama.cpp to `6a2bc8bfb7cd502e5ebc72e36c97a6f848c21c2c` by @localai-bot in https://github.com/mudler/LocalAI/pull/5390
* chore: :arrow_up: Update ggml-org/whisper.cpp to `62dc8f7d7b72ca8e75c57cd6a100712c631fa5d5` by @localai-bot in https://github.com/mudler/LocalAI/pull/5398
* chore: :arrow_up: Update ggml-org/llama.cpp to `b7a17463ec190aeee7b9077c606c910fb4688b84` by @localai-bot in https://github.com/mudler/LocalAI/pull/5399
* chore: :arrow_up: Update ggml-org/llama.cpp to `8e186ef0e764c7a620e402d1f76ebad60bf31c49` by @localai-bot in https://github.com/mudler/LocalAI/pull/5423
* chore: :arrow_up: Update ggml-org/whisper.cpp to `bd1cb0c8e3a04baa411dc12c1325b6a9f12ee7f4` by @localai-bot in https://github.com/mudler/LocalAI/pull/5424
* chore: :arrow_up: Update ggml-org/whisper.cpp to `78b31ca7824500e429ba026c1a9b48e0b41c50cb` by @localai-bot in https://github.com/mudler/LocalAI/pull/5439
* chore: :arrow_up: Update ggml-org/llama.cpp to `8a1d206f1d2b4e45918b589f3165b4be232f7ba8` by @localai-bot in https://github.com/mudler/LocalAI/pull/5440
* chore: :arrow_up: Update ggml-org/whisper.cpp to `13d92d08ae26031545921243256aaaf0ee057943` by @localai-bot in https://github.com/mudler/LocalAI/pull/5449
* chore: :arrow_up: Update ggml-org/llama.cpp to `d13d0f6135803822ec1cd7e3efb49360b88a1bdf` by @localai-bot in https://github.com/mudler/LocalAI/pull/5448
* chore(deps): bump llama.cpp to 'fef693dc6b959a8e8ba11558fbeaad0b264dd457' by @mudler in https://github.com/mudler/LocalAI/pull/5467
* chore: :arrow_up: Update ggml-org/whisper.cpp to `ea9f206f18d86c4eb357db9fdc52e4d9dc24435e` by @localai-bot in https://github.com/mudler/LocalAI/pull/5464
* chore: :arrow_up: Update ggml-org/llama.cpp to `a26c4cc11ec7c6574e3691e90ecdbd67deeea35b` by @localai-bot in https://github.com/mudler/LocalAI/pull/5500
* chore: :arrow_up: Update ggml-org/llama.cpp to `a3c30846e410c91c11d7bf80978795a03bb03dee` by @localai-bot in https://github.com/mudler/LocalAI/pull/5509
* chore: :arrow_up: Update ggml-org/whisper.cpp to `0ed00d9d30e8c984936ff9ed9a4fcd475d6d82e5` by @localai-bot in https://github.com/mudler/LocalAI/pull/5510
* chore: :arrow_up: Update ggml-org/llama.cpp to `d98f2a35fcf4a8d3e660ad48cd19e2a1f3d5b2ef` by @localai-bot in https://github.com/mudler/LocalAI/pull/5514
* chore: :arrow_up: Update ggml-org/whisper.cpp to `1f5fdbecb411a61b8576242e5170c5ecef24b05a` by @localai-bot in https://github.com/mudler/LocalAI/pull/5515
* chore: :arrow_up: Update ggml-org/whisper.cpp to `e5e900dd00747f747143ad30a697c8f21ddcd59e` by @localai-bot in https://github.com/mudler/LocalAI/pull/5522
* chore(deps): bump llama.cpp to 'e83ba3e460651b20a594e9f2f0f0bffb998d3ce1 by @mudler in https://github.com/mudler/LocalAI/pull/5527
* chore: :arrow_up: Update ggml-org/whisper.cpp to `98dfe8dc264b7d0d1daccfff9a9c043bcc2ece4b` by @localai-bot in https://github.com/mudler/LocalAI/pull/5542
* chore(deps): bump llama.cpp to 'e562eece7cb476276bfc4cbb18deb7c0369b2233' by @mudler in https://github.com/mudler/LocalAI/pull/5552
* chore: :arrow_up: Update ggml-org/whisper.cpp to `7fd6fa809749078aa00edf945e959c898f2bd1af` by @localai-bot in https://github.com/mudler/LocalAI/pull/5556
* chore: :arrow_up: Update ggml-org/whisper.cpp to `e05af2457b7b4134ee626dc044294a19b096e62f` by @localai-bot in https://github.com/mudler/LocalAI/pull/5569
* chore(deps): bump llama.cpp to '363757628848a27a435bbf22ff9476e9aeda5f40' by @mudler in https://github.com/mudler/LocalAI/pull/5571
* chore: :arrow_up: Update ggml-org/llama.cpp to `7e00e60ef86645a01fda738fef85b74afa016a34` by @localai-bot in https://github.com/mudler/LocalAI/pull/5574
* chore: :arrow_up: Update ggml-org/whisper.cpp to `82f461eaa4e6a1ba29fc0dbdaa415a9934ee8a1d` by @localai-bot in https://github.com/mudler/LocalAI/pull/5575
* chore(deps): bump GrantBirki/git-diff-action from 2.8.0 to 2.8.1 by @dependabot in https://github.com/mudler/LocalAI/pull/5564
* chore: :arrow_up: Update ggml-org/llama.cpp to `0d3984424f2973c49c4bcabe4cc0153b4f90c601` by @localai-bot in https://github.com/mudler/LocalAI/pull/5585
* chore: :arrow_up: Update ggml-org/whisper.cpp to `799eacdde40b3c562cfce1508da1354b90567f8f` by @localai-bot in https://github.com/mudler/LocalAI/pull/5586
* chore: :arrow_up: Update ggml-org/llama.cpp to `1caae7fc6c77551cb1066515e0f414713eebb367` by @localai-bot in https://github.com/mudler/LocalAI/pull/5593
* chore: :arrow_up: Update ggml-org/whisper.cpp to `b175baa665bc35f97a2ca774174f07dfffb84e19` by @localai-bot in https://github.com/mudler/LocalAI/pull/5597
* chore: :arrow_up: Update ggml-org/llama.cpp to `745aa5319b9930068aff5e87cf5e9eef7227339b` by @localai-bot in https://github.com/mudler/LocalAI/pull/5598
* chore: :arrow_up: Update ggml-org/llama.cpp to `5787b5da57e54dba760c2deeac1edf892e8fc450` by @localai-bot in https://github.com/mudler/LocalAI/pull/5601
* chore: :arrow_up: Update ggml-org/llama.cpp to `247e5c6e447707bb4539bdf1913d206088a8fc69` by @localai-bot in https://github.com/mudler/LocalAI/pull/5605
* chore: :arrow_up: Update ggml-org/whisper.cpp to `d78f08142381c1460604713e2f2ddf3331c7d816` by @localai-bot in https://github.com/mudler/LocalAI/pull/5619
* chore: :arrow_up: Update ggml-org/llama.cpp to `3678b838bb71eaccbaeb479ff38c2e12bfd2f960` by @localai-bot in https://github.com/mudler/LocalAI/pull/5620
* chore: :arrow_up: Update ggml-org/whisper.cpp to `2679bec6e09231c6fd59715fcba3eebc9e2f6076` by @localai-bot in https://github.com/mudler/LocalAI/pull/5625
* chore: :arrow_up: Update ggml-org/whisper.cpp to `ebbc874e85b518f963a87612f6d79f5c71a55e84` by @localai-bot in https://github.com/mudler/LocalAI/pull/5635
* chore: :arrow_up: Update ggml-org/llama.cpp to `ed52f3668e633423054a4eab61bb7efee47025ab` by @localai-bot in https://github.com/mudler/LocalAI/pull/5636
* chore: :arrow_up: Update ggml-org/whisper.cpp to `705db0f728310c32bc96f4e355e2b18076932f75` by @localai-bot in https://github.com/mudler/LocalAI/pull/5643
* chore: :arrow_up: Update ggml-org/llama.cpp to `3cb203c89f60483e349f841684173446ed23c28f` by @localai-bot in https://github.com/mudler/LocalAI/pull/5644
* chore: :arrow_up: Update ggml-org/llama.cpp to `30e5b01de2a0bcddc7c063c8ef0802703a958417` by @localai-bot in https://github.com/mudler/LocalAI/pull/5659
* chore(deps): bump securego/gosec from 2.22.4 to 2.22.5 by @dependabot in https://github.com/mudler/LocalAI/pull/5663
* chore: :arrow_up: Update ggml-org/whisper.cpp to `2a4d6db7d90899aff3d58d70996916968e4e0d27` by @localai-bot in https://github.com/mudler/LocalAI/pull/5661
* chore(deps): bump llama.cpp to 'e434e69183fd9e1031f4445002083178c331a28b by @mudler in https://github.com/mudler/LocalAI/pull/5665
* chore: :arrow_up: Update ggml-org/whisper.cpp to `f3ff80ea8da044e5b8833e7ba54ee174504c518d` by @localai-bot in https://github.com/mudler/LocalAI/pull/5677
* chore: :arrow_up: Update ggml-org/llama.cpp to `860a9e4eeff3eb2e7bd1cc38f65787cc6c8177af` by @localai-bot in https://github.com/mudler/LocalAI/pull/5678
* chore: :arrow_up: Update ggml-org/llama.cpp to `8d947136546773f6410756f37fcc5d3e65b8135d` by @localai-bot in https://github.com/mudler/LocalAI/pull/5685
* chore: :arrow_up: Update ggml-org/whisper.cpp to `ecb8f3c2b4e282d5ef416516bcbfb92821f06bf6` by @localai-bot in https://github.com/mudler/LocalAI/pull/5686
### Other Changes
* docs: :arrow_up: update docs version mudler/LocalAI by @localai-bot in https://github.com/mudler/LocalAI/pull/5363
* chore: memoize detected GPUs by @mudler in https://github.com/mudler/LocalAI/pull/5385
* fix(transformers): pin protobuf by @mudler in https://github.com/mudler/LocalAI/pull/5421
* chore(scripts): allow to specify quants by @mudler in https://github.com/mudler/LocalAI/pull/5430
* fix(transformers): try to pin to working release by @mudler in https://github.com/mudler/LocalAI/pull/5426
* chore(model gallery): add nvidia_acereason-nemotron-14b by @mudler in https://github.com/mudler/LocalAI/pull/5463
* chore(deps): remove pin on transformers by @mudler in https://github.com/mudler/LocalAI/pull/5501
* feat(chatterbox): add new backend by @mudler in https://github.com/mudler/LocalAI/pull/5524
* fix(ci): try to add different mirrors to avoid 403 issues by @mudler in https://github.com/mudler/LocalAI/pull/5554
* Revert "fix(ci): try to add different mirrors to avoid 403 issues" by @mudler in https://github.com/mudler/LocalAI/pull/5555
* chore(deps): bump grpcio from 1.72.0 to 1.72.1 by @mudler in https://github.com/mudler/LocalAI/pull/5570
* fix(chatterbox): install only with cuda 12 by @mudler in https://github.com/mudler/LocalAI/pull/5573
* chore(deps): bump pytorch to 2.7 in vllm by @mudler in https://github.com/mudler/LocalAI/pull/5576
* fix(deps): pin grpcio by @mudler in https://github.com/mudler/LocalAI/pull/5621
* Improve Comments and Documentation for MixedMode and ParseJSON Functions by @leopardracer in https://github.com/mudler/LocalAI/pull/5626
* Fix Typos in Comments and Error Messages by @kilavvy in https://github.com/mudler/LocalAI/pull/5637
* docs: Update docs metadata headers so when mentioned on slack it doesn't say hugo by @halkeye in https://github.com/mudler/LocalAI/pull/5642
* Minor Documentation Updates: Clarified Comments in Python and Go Files by @vtjl10 in https://github.com/mudler/LocalAI/pull/5641
* chore: improve tests by @mudler in https://github.com/mudler/LocalAI/pull/5646
* Fix Typos and Improve Documentation Clarity by @zeevick10 in https://github.com/mudler/LocalAI/pull/5648
* chore(ci): use public runner for extra backends by @mudler in https://github.com/mudler/LocalAI/pull/5657
* chore: Add python3 to images by @mudler in https://github.com/mudler/LocalAI/pull/5660
* fix: add python symlink, use absolute python env path when running backends by @mudler in https://github.com/mudler/LocalAI/pull/5664
* chore(backend gallery): re-order and add description for vLLM by @mudler in https://github.com/mudler/LocalAI/pull/5676
* chore(backend gallery): add description for remaining backends by @mudler in https://github.com/mudler/LocalAI/pull/5679
* chore(ci): switch to public runners for base images by @mudler in https://github.com/mudler/LocalAI/pull/5680
* chore(ci): try to use public runners also for release builds by @mudler in https://github.com/mudler/LocalAI/pull/5681
* chore(ci): move also other jobs to public runner by @mudler in https://github.com/mudler/LocalAI/pull/5683
* Fix Typos in Documentation and Python Comments by @maximevtush in https://github.com/mudler/LocalAI/pull/5658
* Fix Typos and Improve Clarity in GPU Acceleration Documentation by @leopardracer in https://github.com/mudler/LocalAI/pull/5688

&lt;/details&gt;

&lt;h2&gt;New Contributors&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;@omahs made their first contribution in https://github.com/mudler/LocalAI/pull/5376&lt;/li&gt;
&lt;li&gt;@TheDarkTrumpet made their first contribution in https://github.com/mudler/LocalAI/pull/5420&lt;/li&gt;
&lt;li&gt;@halkeye made their first contribution in https://github.com/mudler/LocalAI/pull/5589&lt;/li&gt;
&lt;li&gt;@leopardracer made their first contribution in https://github.com/mudler/LocalAI/pull/5626&lt;/li&gt;
&lt;li&gt;@kilavvy made their first contribution in https://github.com/mudler/LocalAI/pull/5637&lt;/li&gt;
&lt;li&gt;@vtjl10 made their first contribution in https://github.com/mudler/LocalAI/pull/5641&lt;/li&gt;
&lt;li&gt;@zeevick10 made their first contribution in https://github.com/mudler/LocalAI/pull/5648&lt;/li&gt;
&lt;li&gt;@maximevtush made their first contribution in https://github.com/mudler/LocalAI/pull/5658&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Full Changelog&lt;/strong&gt;: https://github.com/mudler/LocalAI/compare/v2.29.0...v3.0.0&lt;/p&gt;</content>
    <link href="https://github.com/mudler/LocalAI/releases/tag/v3.0.0" rel="alternate"/>
  </entry>
  <entry>
    <id>https://github.com/mudler/LocalAI/releases/tag/v3.1.0</id>
    <title>New release for LocalAI: v3.1.0</title>
    <updated>2025-06-26T16:14:10-04:00</updated>
    <author>
      <name>mudler/LocalAI</name>
    </author>
    <content>&lt;h1 align="center"&gt;
  &lt;br&gt;
  &lt;img height="300" src="https://raw.githubusercontent.com/mudler/LocalAI/refs/heads/master/core/http/static/logo.png"&gt; &lt;br&gt;
&lt;br&gt;
üöÄ LocalAI 3.1
&lt;/h1&gt;

&lt;h1&gt;üöÄ Highlights&lt;/h1&gt;
&lt;h2&gt;Support for Gemma 3n!&lt;/h2&gt;
&lt;p&gt;Gemma 3n has been released and it's now available in LocalAI (currently only for text generation, install it with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;local-ai run gemma-3n-e2b-it&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;local-ai run gemma-3n-e4b-it&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;‚ö†Ô∏è Breaking Changes&lt;/h2&gt;
&lt;p&gt;Several important changes that reduce image size, simplify the ecosystem, and pave the way for a leaner LocalAI core:&lt;/p&gt;
&lt;h3&gt;üß∞ Container Image Changes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Sources are no longer bundled in the container images. This significantly reduces image sizes.&lt;/li&gt;
&lt;li&gt;Need to rebuild locally? Just follow the docs to build from scratch. We're working towards migrating all backends to the gallery, slimming down the default image further.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;üìÅ Directory Structure Updated&lt;/h3&gt;
&lt;p&gt;New default model and backend paths for container images:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Models: &lt;code&gt;/models/&lt;/code&gt; (was &lt;code&gt;/build/models&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Backends: &lt;code&gt;/backends/&lt;/code&gt; (was &lt;code&gt;/build/backends&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;üè∑ Unified Image Tag Naming for &lt;code&gt;master&lt;/code&gt; (development) builds&lt;/h3&gt;
&lt;p&gt;We've cleaned up and standardized container image tags for clarity and consistency:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;gpu-nvidia-cuda11&lt;/code&gt; and &lt;code&gt;gpu-nvidia-cuda12&lt;/code&gt; (previously &lt;code&gt;cublas-cuda11&lt;/code&gt;, &lt;code&gt;cublas-cuda12&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gpu-intel-f16&lt;/code&gt; and &lt;code&gt;gpu-intel-f32&lt;/code&gt; (previously &lt;code&gt;sycl-f16&lt;/code&gt;, &lt;code&gt;sycl-f32&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Meta packages in backend galleries&lt;/h2&gt;
&lt;p&gt;We‚Äôve introduced meta-packages to the backend gallery!
These packages automatically install the most suitable backend depending on the GPU detected in your system ‚Äî saving time, reducing errors, and ensuring you get the right setup out of the box. These will be added as soon as the 3.1.0 images are going to be published, stay tuned!&lt;/p&gt;
&lt;p&gt;For instance, you will be able to install &lt;code&gt;vllm&lt;/code&gt; just by installing the &lt;code&gt;vllm&lt;/code&gt; backend in the gallery ( no need to select anymore the correct GPU version)&lt;/p&gt;
&lt;h2&gt;The Complete Local Stack for Privacy-First AI&lt;/h2&gt;
&lt;p&gt;With LocalAGI rejoining LocalAI alongside LocalRecall, our ecosystem provides a complete, open-source stack for private, secure, and intelligent AI operations:&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td width="30%" valign="top" align="center"&gt;
      &lt;a href="https://github.com/mudler/LocalAI"&gt;
        &lt;img src="https://raw.githubusercontent.com/mudler/LocalAI/refs/heads/master/core/http/static/logo.png" width="200" alt="LocalAI Logo"&gt;
        &lt;h3&gt;LocalAI&lt;/h3&gt;
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td width="70%" valign="top"&gt;
      &lt;p&gt;The free, Open Source OpenAI alternative. Acts as a drop-in replacement REST API compatible with OpenAI specifications for local AI inferencing. No GPU required.&lt;/p&gt;
      &lt;p&gt;&lt;em&gt;Link:&lt;/em&gt; &lt;a href="https://github.com/mudler/LocalAI"&gt;https://github.com/mudler/LocalAI&lt;/a&gt;&lt;/p&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width="30%" valign="top" align="center"&gt;
      &lt;a href="https://github.com/mudler/LocalAGI"&gt;
         &lt;img src="https://raw.githubusercontent.com/mudler/LocalAGI/refs/heads/main/webui/react-ui/public/logo_2.png" width="200" alt="LocalAGI Logo"&gt;
         &lt;h3&gt;LocalAGI&lt;/h3&gt;
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td width="70%" valign="top"&gt;
      &lt;p&gt;A powerful Local AI agent management platform. Serves as a drop-in replacement for OpenAI's Responses API, supercharged with advanced agentic capabilities and a no-code UI.&lt;/p&gt;
      &lt;p&gt;&lt;em&gt;Link:&lt;/em&gt; &lt;a href="https://github.com/mudler/LocalAGI"&gt;https://github.com/mudler/LocalAGI&lt;/a&gt;&lt;/p&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width="30%" valign="top" align="center"&gt;
      &lt;a href="https://github.com/mudler/LocalRecall"&gt;
         &lt;img src="https://raw.githubusercontent.com/mudler/LocalRecall/refs/heads/main/static/localrecall_horizontal.png" width="200" alt="LocalRecall Logo"&gt;
         &lt;h3&gt;LocalRecall&lt;/h3&gt;
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td width="70%" valign="top"&gt;
      &lt;p&gt;A RESTful API and knowledge base management system providing persistent memory and storage capabilities for AI agents. Designed to work alongside LocalAI and LocalAGI.&lt;/p&gt;
      &lt;p&gt;&lt;em&gt;Link:&lt;/em&gt; &lt;a href="https://github.com/mudler/LocalRecall"&gt;https://github.com/mudler/LocalRecall&lt;/a&gt;&lt;/p&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h2&gt;Join the Movement! ‚ù§Ô∏è&lt;/h2&gt;
&lt;p&gt;A massive &lt;strong&gt;THANK YOU&lt;/strong&gt; to our incredible community and our sponsors! LocalAI has over &lt;strong&gt;33,500 stars&lt;/strong&gt;, and LocalAGI has already rocketed past &lt;strong&gt;800+ stars&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;As a reminder, LocalAI is real FOSS (Free and Open Source Software) and its sibling projects are community-driven and not backed by VCs or a company. We rely on contributors donating their spare time and our sponsors to provide us the hardware! If you love open-source, privacy-first AI, please consider starring the repos, contributing code, reporting bugs, or spreading the word!&lt;/p&gt;
&lt;p&gt;üëâ &lt;strong&gt;Check out the reborn LocalAGI v2 today:&lt;/strong&gt; &lt;a href="https://github.com/mudler/LocalAGI"&gt;https://github.com/mudler/LocalAGI&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Full changelog :point_down:&lt;/h2&gt;
&lt;details&gt;

&lt;summary&gt;
:point_right: Click to expand :point_left: 
&lt;/summary&gt;


## What's Changed
### Breaking Changes üõ†
* chore(ci): :warning: fix latest tag by using docker meta action by @mudler in https://github.com/mudler/LocalAI/pull/5722
* feat: :warning:  reduce images size and stop bundling sources by @mudler in https://github.com/mudler/LocalAI/pull/5721
### Bug fixes :bug:
* fix(backends gallery): delete dangling dirs if installation failed by @mudler in https://github.com/mudler/LocalAI/pull/5729
### Exciting New Features üéâ
* feat(backend gallery): add meta packages by @mudler in https://github.com/mudler/LocalAI/pull/5696
### üß† Models
* chore(model gallery): add qwen3-the-josiefied-omega-directive-22b-uncensored-abliterated-i1 by @mudler in https://github.com/mudler/LocalAI/pull/5704
* chore(model gallery): add menlo_jan-nano by @mudler in https://github.com/mudler/LocalAI/pull/5705
* chore(model gallery): add qwen3-the-xiaolong-omega-directive-22b-uncensored-abliterated-i1 by @mudler in https://github.com/mudler/LocalAI/pull/5706
* chore(model gallery): add allura-org_q3-8b-kintsugi by @mudler in https://github.com/mudler/LocalAI/pull/5707
* chore(model gallery): add ds-r1-qwen3-8b-arliai-rpr-v4-small-iq-imatrix by @mudler in https://github.com/mudler/LocalAI/pull/5708
* chore(model gallery): add mistralai_mistral-small-3.2-24b-instruct-2506 by @mudler in https://github.com/mudler/LocalAI/pull/5714
* chore(model gallery): add skywork_skywork-swe-32b by @mudler in https://github.com/mudler/LocalAI/pull/5715
* chore(model gallery): add astrosage-70b by @mudler in https://github.com/mudler/LocalAI/pull/5716
* chore(model gallery): add delta-vector_austral-24b-winton by @mudler in https://github.com/mudler/LocalAI/pull/5717
* chore(model gallery): add menlo_jan-nano-128k by @mudler in https://github.com/mudler/LocalAI/pull/5723
* chore(model gallery): add gemma-3n-e2b-it by @mudler in https://github.com/mudler/LocalAI/pull/5730
* chore(model gallery): add gemma-3n-e4b-it by @mudler in https://github.com/mudler/LocalAI/pull/5731
### üëí Dependencies
* chore: :arrow_up: Update ggml-org/whisper.cpp to `3e65f518ddf840b13b74794158aa95a2c8aa30cc` by @localai-bot in https://github.com/mudler/LocalAI/pull/5691
* chore: :arrow_up: Update ggml-org/llama.cpp to `8f71d0f3e86ccbba059350058af8758cafed73e6` by @localai-bot in https://github.com/mudler/LocalAI/pull/5692
* chore: :arrow_up: Update ggml-org/llama.cpp to `06cbedfca1587473df9b537f1dd4d6bfa2e3de13` by @localai-bot in https://github.com/mudler/LocalAI/pull/5697
* chore: :arrow_up: Update ggml-org/whisper.cpp to `e6c10cf3d5d60dc647eb6cd5e73d3c347149f746` by @localai-bot in https://github.com/mudler/LocalAI/pull/5702
* chore: :arrow_up: Update ggml-org/llama.cpp to `aa0ef5c578eef4c2adc7be1282f21bab5f3e8d26` by @localai-bot in https://github.com/mudler/LocalAI/pull/5703
* chore: :arrow_up: Update ggml-org/llama.cpp to `238005c2dc67426cf678baa2d54c881701693288` by @localai-bot in https://github.com/mudler/LocalAI/pull/5710
* chore: :arrow_up: Update ggml-org/whisper.cpp to `a422176937c5bb20eb58d969995765f90d3c1a9b` by @localai-bot in https://github.com/mudler/LocalAI/pull/5713
* chore: :arrow_up: Update ggml-org/llama.cpp to `ce82bd0117bd3598300b3a089d13d401b90279c7` by @localai-bot in https://github.com/mudler/LocalAI/pull/5712
* chore: :arrow_up: Update ggml-org/llama.cpp to `73e53dc834c0a2336cd104473af6897197b96277` by @localai-bot in https://github.com/mudler/LocalAI/pull/5719
* chore: :arrow_up: Update ggml-org/whisper.cpp to `0083335ba0e9d6becbe0958903b0a27fc2ebaeed` by @localai-bot in https://github.com/mudler/LocalAI/pull/5718
* chore: :arrow_up: Update leejet/stable-diffusion.cpp to `10c6501bd05a697e014f1bee3a84e5664290c489` by @localai-bot in https://github.com/mudler/LocalAI/pull/4925
* chore: :arrow_up: Update ggml-org/llama.cpp to `2bf9d539dd158345e3a3b096e16474af535265b4` by @localai-bot in https://github.com/mudler/LocalAI/pull/5724
* chore: :arrow_up: Update ggml-org/whisper.cpp to `4daf7050ca2bf17f5166f45ac6da651c4e33f293` by @localai-bot in https://github.com/mudler/LocalAI/pull/5725
* Revert "chore: :arrow_up: Update leejet/stable-diffusion.cpp to `10c6501bd05a697e014f1bee3a84e5664290c489`" by @mudler in https://github.com/mudler/LocalAI/pull/5727
* chore: :arrow_up: Update ggml-org/llama.cpp to `8846aace4934ad29651ea61b8c7e3f6b0556e3d2` by @localai-bot in https://github.com/mudler/LocalAI/pull/5734
* chore: :arrow_up: Update ggml-org/whisper.cpp to `32cf4e2aba799aff069011f37ca025401433cf9f` by @localai-bot in https://github.com/mudler/LocalAI/pull/5733
### Other Changes
* docs: :arrow_up: update docs version mudler/LocalAI by @localai-bot in https://github.com/mudler/LocalAI/pull/5690
* chore(ci): try to optimize disk space when tagging latest by @mudler in https://github.com/mudler/LocalAI/pull/5695
* chore(ci): add stale bot by @mudler in https://github.com/mudler/LocalAI/pull/5700
* Docs: Fix typos by @kilavvy in https://github.com/mudler/LocalAI/pull/5709
&lt;/details&gt;

&lt;p&gt;&lt;strong&gt;Full Changelog&lt;/strong&gt;: https://github.com/mudler/LocalAI/compare/v3.0.0...v3.1.0&lt;/p&gt;</content>
    <link href="https://github.com/mudler/LocalAI/releases/tag/v3.1.0" rel="alternate"/>
  </entry>
  <entry>
    <id>https://github.com/mudler/LocalAI/releases/tag/v3.1.1</id>
    <title>New release for LocalAI: v3.1.1</title>
    <updated>2025-06-27T17:48:46-04:00</updated>
    <author>
      <name>mudler/LocalAI</name>
    </author>
    <content>&lt;!-- Release notes generated using configuration in .github/release.yml at master --&gt;

&lt;h2&gt;What's Changed&lt;/h2&gt;
&lt;h3&gt;Bug fixes :bug:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;fix(backends gallery): correctly identify gpu vendor by @mudler in https://github.com/mudler/LocalAI/pull/5739&lt;/li&gt;
&lt;li&gt;fix(backends gallery): meta packages do not have URIs by @mudler in https://github.com/mudler/LocalAI/pull/5740&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Exciting New Features üéâ&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;feat(gallery): automatically install missing backends along models by @mudler in https://github.com/mudler/LocalAI/pull/5736&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;üëí Dependencies&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;chore: :arrow_up: Update ggml-org/whisper.cpp to &lt;code&gt;c88ffbf9baeaae8c2cc0a4f496618314bb2ee9e0&lt;/code&gt; by @localai-bot in https://github.com/mudler/LocalAI/pull/5742&lt;/li&gt;
&lt;li&gt;chore: :arrow_up: Update ggml-org/llama.cpp to &lt;code&gt;72babea5dea56c8a8e8420ccf731b12a5cf37854&lt;/code&gt; by @localai-bot in https://github.com/mudler/LocalAI/pull/5743&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Other Changes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;fix(ci): better handling of latest images for backends by @mudler in https://github.com/mudler/LocalAI/pull/5735&lt;/li&gt;
&lt;li&gt;fix(ci): enable tag-latest to auto by @mudler in https://github.com/mudler/LocalAI/pull/5738&lt;/li&gt;
&lt;li&gt;docs: :arrow_up: update docs version mudler/LocalAI by @localai-bot in https://github.com/mudler/LocalAI/pull/5741&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Full Changelog&lt;/strong&gt;: https://github.com/mudler/LocalAI/compare/v3.1.0...v3.1.1&lt;/p&gt;</content>
    <link href="https://github.com/mudler/LocalAI/releases/tag/v3.1.1" rel="alternate"/>
  </entry>
  <entry>
    <id>https://github.com/mudler/LocalAI/releases/tag/v3.2.0</id>
    <title>New release for LocalAI: v3.2.0</title>
    <updated>2025-07-24T16:36:41-04:00</updated>
    <author>
      <name>mudler/LocalAI</name>
    </author>
    <content>&lt;h1 align="center"&gt;
  &lt;br&gt;
  &lt;img height="300" src="https://raw.githubusercontent.com/mudler/LocalAI/refs/heads/master/core/http/static/logo.png"&gt; &lt;br&gt;
&lt;br&gt;
üöÄ LocalAI 3.2.0
&lt;/h1&gt;

&lt;p&gt;Welcome to LocalAI 3.2.0! This is a release that refactors our architecture to be more flexible and lightweight. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The core is now separated from all the backends&lt;/strong&gt;, making LocalAI faster to download, easier to manage, portable, and much more smaller.&lt;/p&gt;
&lt;h2&gt;TL;DR ‚Äì What‚Äôs New in LocalAI 3.2.0 üéâ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;üß© Modular Backends: All backends now live outside the main binary in our new Backend Gallery. This means you can update, add, or manage backends independently of LocalAI releases.&lt;/li&gt;
&lt;li&gt;üìâ Leaner Than Ever: The LocalAI binary and container images are drastically smaller, making for faster downloads and a reduced footprint.&lt;/li&gt;
&lt;li&gt;ü§ñ Smart Backend Installation: It just works! When you install a model, LocalAI automatically detects your hardware (CPU, NVIDIA, AMD, Intel) and downloads the necessary backend. No more manual configuration!&lt;/li&gt;
&lt;li&gt;üõ†Ô∏è Simplified Build Process: The new modular architecture significantly simplifies the build process for contributors and power users.&lt;/li&gt;
&lt;li&gt;‚ö°Ô∏è Intel GPU Support for Whisper: Transcription with Whisper can now be accelerated on Intel GPUs using SYCL, bringing more hardware options to our users.&lt;/li&gt;
&lt;li&gt;üó£Ô∏è Enhanced Realtime Audio: We've added speech started and stopped events for more interactive applications and OpenAI-compatible support for the input_audio field in the chat API.&lt;/li&gt;
&lt;li&gt;üß† Massive Model Expansion: The gallery has been updated with over 50 new models, including the latest from &lt;code&gt;Qwen3&lt;/code&gt;, &lt;code&gt;Gemma&lt;/code&gt;, &lt;code&gt;Mistral&lt;/code&gt;, &lt;code&gt;Nemotron&lt;/code&gt;, and more!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: CI is in the process of building all the backends for this release and will be available soon - if you hit any issue, please try in a few, thanks for understanding!
&lt;strong&gt;Note&lt;/strong&gt;: Some parts of the documentation and the installation scripts (that download the release binaries) have to yet be adapted to the latest changes and/or might not reflect the current state&lt;/p&gt;
&lt;h2&gt;A New Modular Architecture üß©&lt;/h2&gt;
&lt;p&gt;The biggest change in v3.2.0 is the complete separation of inference backends from the core LocalAI binary. Backends like llama.cpp, whisper.cpp, piper, and stablediffusion-ggml are no longer bundled in.&lt;/p&gt;
&lt;p&gt;This fundamental shift makes LocalAI:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lighter: Significantly smaller binary and container image sizes.&lt;/li&gt;
&lt;li&gt;More Flexible: Update backends anytime from the gallery without waiting for a new LocalAI release.&lt;/li&gt;
&lt;li&gt;Easier to Maintain: A cleaner, more streamlined codebase for faster development.&lt;/li&gt;
&lt;li&gt;Easier to Customize: you can build your own backends and install them in your LocalAI instances.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Smart, Automatic Backend Installation ü§ñ&lt;/h2&gt;
&lt;p&gt;To make the new modular system seamless, LocalAI now features automatic backend installation.&lt;/p&gt;
&lt;p&gt;When you install a model from the gallery (or a YAML file), LocalAI intelligently detects the required backend and your system's capabilities, then downloads the correct version for you. Whether you're running on a standard CPU, an NVIDIA GPU, an AMD GPU, or an Intel GPU, LocalAI handles it automatically.&lt;/p&gt;
&lt;p&gt;For advanced use cases or to override auto-detection, you can use the LOCALAI_FORCE_META_BACKEND_CAPABILITY environment variable. Here are the available options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;default&lt;/code&gt;: Forces CPU-only backend. This is the fallback if no specific hardware is detected.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nvidia&lt;/code&gt;: Forces backends compiled with CUDA support for NVIDIA GPUs.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;amd&lt;/code&gt;: Forces backends compiled with ROCm support for AMD GPUs.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;intel&lt;/code&gt;: Forces backends compiled with SYCL/oneAPI support for Intel GPUs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;The Backend Gallery &amp;amp; CLI Control üñºÔ∏è&lt;/h2&gt;
&lt;p&gt;You are in full control. You can browse, install, and manage all available backends directly from the WebUI or using the new CLI commands:&lt;/p&gt;
&lt;p&gt;```bash&lt;/p&gt;
&lt;h1&gt;List all available backends in the gallery&lt;/h1&gt;
&lt;p&gt;local-ai backends list&lt;/p&gt;
&lt;h1&gt;Install a specific backend (e.g., llama-cpp)&lt;/h1&gt;
&lt;p&gt;local-ai backends install llama-cpp&lt;/p&gt;
&lt;h1&gt;Uninstall a backend&lt;/h1&gt;
&lt;p&gt;local-ai backends uninstall llama-cpp
```&lt;/p&gt;
&lt;p&gt;For development, offline or air-gapped environments, you can now also install backends directly from a local OCI tar file:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;local-ai backends install "ocifile://&amp;lt;PATH_TO_TAR_FILE&amp;gt;"&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;Other Key Improvements&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;üó£Ô∏è Enhanced Realtime and Audio APIs: Building voice-activated applications is now easier.&lt;/li&gt;
&lt;li&gt;The new speech started and stopped events give you precise control over realtime audio streams.&lt;/li&gt;
&lt;li&gt;We now support the input_audio field in the /v1/chat/completions endpoint for multimodal audio inputs, improving OpenAI compatibility.&lt;/li&gt;
&lt;li&gt;‚ö°Ô∏è Intel GPU Acceleration for Whisper: Our Whisper backend now supports SYCL, enabling hardware-accelerated transcriptions on Intel GPUs.&lt;/li&gt;
&lt;li&gt;‚úÖ UI and Bug Fixes: We've squashed several bugs for a smoother experience, including a fix that correctly shows the download status for backend images in the gallery, so you always know what's happening.&lt;/li&gt;
&lt;li&gt;üß† Massive Model Gallery Expansion: Our model gallery has never been bigger! We've added over 50 new and updated models, with a focus on powerful new releases like qwen3, devstral-small, and nemotron.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;üö® Important Note for Upgrading&lt;/h2&gt;
&lt;p&gt;Due to the new modular architecture, if you have existing models installed with a version prior to 3.2.0, they might not have a specific backend assigned.&lt;/p&gt;
&lt;p&gt;After upgrading, you may need to install the required backend manually for these models to work. You can do this easily from the WebUI or via the CLI: &lt;code&gt;local-ai backends install &amp;lt;backend_name&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;The Complete Local Stack for Privacy-First AI&lt;/h2&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td width="30%" valign="top" align="center"&gt;
      &lt;a href="https://github.com/mudler/LocalAI"&gt;
        &lt;img src="https://raw.githubusercontent.com/mudler/LocalAI/refs/heads/master/core/http/static/logo.png" width="200" alt="LocalAI Logo"&gt;
        &lt;h3&gt;LocalAI&lt;/h3&gt;
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td width="70%" valign="top"&gt;
      &lt;p&gt;The free, Open Source OpenAI alternative. Acts as a drop-in replacement REST API compatible with OpenAI specifications for local AI inferencing. No GPU required.&lt;/p&gt;
      &lt;p&gt;&lt;em&gt;Link:&lt;/em&gt; &lt;a href="https://github.com/mudler/LocalAI"&gt;https://github.com/mudler/LocalAI&lt;/a&gt;&lt;/p&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width="30%" valign="top" align="center"&gt;
      &lt;a href="https://github.com/mudler/LocalAGI"&gt;
         &lt;img src="https://raw.githubusercontent.com/mudler/LocalAGI/refs/heads/main/webui/react-ui/public/logo_2.png" width="200" alt="LocalAGI Logo"&gt;
         &lt;h3&gt;LocalAGI&lt;/h3&gt;
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td width="70%" valign="top"&gt;
      &lt;p&gt;A powerful Local AI agent management platform. Serves as a drop-in replacement for OpenAI's Responses API, supercharged with advanced agentic capabilities and a no-code UI.&lt;/p&gt;
      &lt;p&gt;&lt;em&gt;Link:&lt;/em&gt; &lt;a href="https://github.com/mudler/LocalAGI"&gt;https://github.com/mudler/LocalAGI&lt;/a&gt;&lt;/p&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width="30%" valign="top" align="center"&gt;
      &lt;a href="https://github.com/mudler/LocalRecall"&gt;
         &lt;img src="https://raw.githubusercontent.com/mudler/LocalRecall/refs/heads/main/static/localrecall_horizontal.png" width="200" alt="LocalRecall Logo"&gt;
         &lt;h3&gt;LocalRecall&lt;/h3&gt;
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td width="70%" valign="top"&gt;
      &lt;p&gt;A RESTful API and knowledge base management system providing persistent memory and storage capabilities for AI agents. Designed to work alongside LocalAI and LocalAGI.&lt;/p&gt;
      &lt;p&gt;&lt;em&gt;Link:&lt;/em&gt; &lt;a href="https://github.com/mudler/LocalRecall"&gt;https://github.com/mudler/LocalRecall&lt;/a&gt;&lt;/p&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h2&gt;Thank you! ‚ù§Ô∏è&lt;/h2&gt;
&lt;p&gt;A massive &lt;strong&gt;THANK YOU&lt;/strong&gt; to our incredible community and our sponsors! LocalAI has over &lt;strong&gt;34,100 stars&lt;/strong&gt;, and LocalAGI has already rocketed past &lt;strong&gt;900+ stars&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;As a reminder, LocalAI is real FOSS (Free and Open Source Software) and its sibling projects are community-driven and not backed by VCs or a company. We rely on contributors donating their spare time and our sponsors to provide us the hardware! If you love open-source, privacy-first AI, please consider starring the repos, contributing code, reporting bugs, or spreading the word!&lt;/p&gt;
&lt;p&gt;üëâ &lt;strong&gt;Check out the reborn LocalAGI v2 today:&lt;/strong&gt; &lt;a href="https://github.com/mudler/LocalAGI"&gt;https://github.com/mudler/LocalAGI&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Full changelog :point_down:&lt;/h2&gt;
&lt;details&gt;

&lt;summary&gt;
:point_right: Click to expand :point_left: 
&lt;/summary&gt;


## What's Changed
### Breaking Changes üõ†
* feat: do not bundle llama-cpp anymore by @mudler in https://github.com/mudler/LocalAI/pull/5790
* feat: refactor build process, drop embedded backends by @mudler in https://github.com/mudler/LocalAI/pull/5875
### Bug fixes :bug:
* fix(gallery): automatically install model from name by @mudler in https://github.com/mudler/LocalAI/pull/5757
* fix: Diffusers and XPU fixes by @richiejp in https://github.com/mudler/LocalAI/pull/5737
* fix(gallery): correctly show status for downloading OCI images by @mudler in https://github.com/mudler/LocalAI/pull/5774
* fix: explorer page should not have login by @mudler in https://github.com/mudler/LocalAI/pull/5855
* fix: dockerfile typo by @LeonSijiaLu in https://github.com/mudler/LocalAI/pull/5823
* fix(docs): Resolve logo overlap on tablet view by @dedyf5 in https://github.com/mudler/LocalAI/pull/5853
* fix: do not pass by environ to ffmpeg by @mudler in https://github.com/mudler/LocalAI/pull/5871
* fix(p2p): adapt to backend changes, general improvements by @mudler in https://github.com/mudler/LocalAI/pull/5889
### Exciting New Features üéâ
* feat(llama.cpp): allow to set kv-overrides by @mudler in https://github.com/mudler/LocalAI/pull/5745
* feat(backends): add metas in the gallery by @mudler in https://github.com/mudler/LocalAI/pull/5784
* feat(system): detect and allow to override capabilities by @mudler in https://github.com/mudler/LocalAI/pull/5785
* chore(cli): add backends CLI to manipulate and install backends by @mudler in https://github.com/mudler/LocalAI/pull/5787
* feat(whisper): Enable SYCL by @richiejp in https://github.com/mudler/LocalAI/pull/5802
* feat(cli): allow to install backends from OCI tar files by @mudler in https://github.com/mudler/LocalAI/pull/5816
* feat(cli): add command to create custom OCI images from directories by @mudler in https://github.com/mudler/LocalAI/pull/5844
* feat(realtime): Add speech started and stopped events by @richiejp in https://github.com/mudler/LocalAI/pull/5856
* fix: autoload backends when installing models from YAML files by @mudler in https://github.com/mudler/LocalAI/pull/5859
* feat: split piper from main binary by @mudler in https://github.com/mudler/LocalAI/pull/5858
* feat: remove stablediffusion-ggml from main binary by @mudler in https://github.com/mudler/LocalAI/pull/5861
* feat: split whisper from main binary by @mudler in https://github.com/mudler/LocalAI/pull/5863
* feat(openai): support input_audio chat api field by @mgoltzsche in https://github.com/mudler/LocalAI/pull/5870
* fix(realtime): Reset speech started flag on commit by @richiejp in https://github.com/mudler/LocalAI/pull/5879
* fix(build): Add and update ONEAPI_VERSION by @richiejp in https://github.com/mudler/LocalAI/pull/5874
### üß† Models
* chore(model gallery): add qwen3-55b-a3b-total-recall-v1.3-i1 by @mudler in https://github.com/mudler/LocalAI/pull/5746
* chore(model gallery): add qwen3-55b-a3b-total-recall-deep-40x by @mudler in https://github.com/mudler/LocalAI/pull/5747
* chore(model gallery): add qwen3-42b-a3b-stranger-thoughts-deep20x-abliterated-uncensored-i1 by @mudler in https://github.com/mudler/LocalAI/pull/5748
* chore(model gallery): add mistral-small-3.2-46b-the-brilliant-raconteur-ii-instruct-2506 by @mudler in https://github.com/mudler/LocalAI/pull/5749
* chore(model gallery): add qwen3-22b-a3b-the-harley-quinn by @mudler in https://github.com/mudler/LocalAI/pull/5750
* chore(model gallery): add gemma-3-4b-it-max-horror-uncensored-dbl-x-imatrix by @mudler in https://github.com/mudler/LocalAI/pull/5751
* chore(model gallery): add qwen3-33b-a3b-stranger-thoughts-abliterated-uncensored by @mudler in https://github.com/mudler/LocalAI/pull/5755
* chore(model gallery): add thedrummer_anubis-70b-v1.1 by @mudler in https://github.com/mudler/LocalAI/pull/5771
* chore(model gallery): add steelskull_l3.3-shakudo-70b by @mudler in https://github.com/mudler/LocalAI/pull/5772
* chore(model gallery): add pinkpixel_crystal-think-v2 by @mudler in https://github.com/mudler/LocalAI/pull/5773
* chore(model gallery): add helpingai_dhanishtha-2.0-preview by @mudler in https://github.com/mudler/LocalAI/pull/5791
* chore(model gallery): add agentica-org_deepswe-preview by @mudler in https://github.com/mudler/LocalAI/pull/5792
* chore(model gallery): add zerofata_ms3.2-paintedfantasy-visage-33b by @mudler in https://github.com/mudler/LocalAI/pull/5793
* chore(model gallery): add ockerman0_anubislemonade-70b-v1 by @mudler in https://github.com/mudler/LocalAI/pull/5794
* chore(model gallery): add sicariussicariistuff_impish_llama_4b by @mudler in https://github.com/mudler/LocalAI/pull/5799
* chore(model gallery): add nano_imp_1b-q8_0 by @mudler in https://github.com/mudler/LocalAI/pull/5800
* chore(model gallery): add compumacy-experimental-32b by @mudler in https://github.com/mudler/LocalAI/pull/5803
* chore(model gallery): add mini-hydra by @mudler in https://github.com/mudler/LocalAI/pull/5804
* chore(model gallery): add zonui-3b-i1 by @mudler in https://github.com/mudler/LocalAI/pull/5805
* chore(model gallery): add huihui-jan-nano-abliterated by @mudler in https://github.com/mudler/LocalAI/pull/5806
* chore(model gallery): add cognitivecomputations_dolphin-mistral-24b-venice-edition by @mudler in https://github.com/mudler/LocalAI/pull/5813
* chore(model gallery): add ockerman0_anubislemonade-70b-v1.1 by @mudler in https://github.com/mudler/LocalAI/pull/5814
* chore(model gallery): add qwen3-8b-shiningvaliant3 by @mudler in https://github.com/mudler/LocalAI/pull/5815
* chore(model gallery): add lyranovaheart_starfallen-snow-fantasy-24b-ms3.2-v0.0 by @mudler in https://github.com/mudler/LocalAI/pull/5818
* chore(model gallery): add zerofata_l3.3-geneticlemonade-opus-70b by @mudler in https://github.com/mudler/LocalAI/pull/5819
* chore(model gallery): add huggingfacetb_smollm3-3b by @mudler in https://github.com/mudler/LocalAI/pull/5820
* chore(model gallery): add delta-vector_plesio-70b by @mudler in https://github.com/mudler/LocalAI/pull/5825
* chore(model gallery): add thedrummer_big-tiger-gemma-27b-v3 by @mudler in https://github.com/mudler/LocalAI/pull/5826
* chore(model gallery): add thedrummer_tiger-gemma-12b-v3 by @mudler in https://github.com/mudler/LocalAI/pull/5827
* chore(model gallery): add microsoft_nextcoder-32b by @mudler in https://github.com/mudler/LocalAI/pull/5832
* chore(model gallery): add huihui-ai_huihui-gemma-3n-e4b-it-abliterated by @mudler in https://github.com/mudler/LocalAI/pull/5833
* chore(model gallery): add mistralai_devstral-small-2507 by @mudler in https://github.com/mudler/LocalAI/pull/5834
* chore(model gallery): add nvidia_llama-3_3-nemotron-super-49b-genrm-multilingual by @mudler in https://github.com/mudler/LocalAI/pull/5837
* chore(model gallery): add mistral-2x24b-moe-power-coder-magistral-devstral-reasoning-ultimate-neo-max-44b by @mudler in https://github.com/mudler/LocalAI/pull/5838
* chore(model gallery): add impish_magic_24b-i1 by @mudler in https://github.com/mudler/LocalAI/pull/5839
* chore(model gallery): add google_medgemma-4b-it by @mudler in https://github.com/mudler/LocalAI/pull/5842
* chore(model gallery): add google_medgemma-27b-it by @mudler in https://github.com/mudler/LocalAI/pull/5843
* chore(model gallery): add zhi-create-qwen3-32b-i1 by @mudler in https://github.com/mudler/LocalAI/pull/5847
* chore(model gallery): add sophosympatheia_strawberrylemonade-70b-v1.1 by @mudler in https://github.com/mudler/LocalAI/pull/5848
* chore(model-gallery): :arrow_up: update checksum by @localai-bot in https://github.com/mudler/LocalAI/pull/5865
* chore(model gallery): add omega-qwen3-atom-8b by @mudler in https://github.com/mudler/LocalAI/pull/5883
* chore(model gallery): add dream-org_dream-v0-instruct-7b by @mudler in https://github.com/mudler/LocalAI/pull/5884
* chore(model gallery): add entfane_math-genius-7b by @mudler in https://github.com/mudler/LocalAI/pull/5885
* chore(model gallery): add menlo_lucy by @mudler in https://github.com/mudler/LocalAI/pull/5886
* chore(model gallery): add qwen3-235b-a22b-instruct-2507 by @mudler in https://github.com/mudler/LocalAI/pull/5887
* chore(model gallery): add qwen3-coder-480b-a35b-instruct by @mudler in https://github.com/mudler/LocalAI/pull/5888
### üìñ Documentation and examples
* fix(docs): Improve Header Responsiveness - Hide "Star us on GitHub!" on Mobile by @dedyf5 in https://github.com/mudler/LocalAI/pull/5770
### üëí Dependencies
* chore: :arrow_up: Update ggml-org/llama.cpp to `27208bf657cfe7262791df473927225e48efe482` by @localai-bot in https://github.com/mudler/LocalAI/pull/5753
* chore: :arrow_up: Update ggml-org/llama.cpp to `caf5681fcb47dfe9bafee94ef9aa8f669ac986c7` by @localai-bot in https://github.com/mudler/LocalAI/pull/5758
* chore: :arrow_up: Update ggml-org/llama.cpp to `0a5a3b5cdfd887cf0f8e09d9ff89dee130cfcdde` by @localai-bot in https://github.com/mudler/LocalAI/pull/5759
* chore: :arrow_up: Update ggml-org/whisper.cpp to `bca021c9740b267c2973fba56555be052006023a` by @localai-bot in https://github.com/mudler/LocalAI/pull/5776
* chore: :arrow_up: Update ggml-org/llama.cpp to `de569441470332ff922c23fb0413cc957be75b25` by @localai-bot in https://github.com/mudler/LocalAI/pull/5777
* chore: :arrow_up: Update ggml-org/whisper.cpp to `d9999d54c868b8bfcd376aa26067e787d53e679e` by @localai-bot in https://github.com/mudler/LocalAI/pull/5782
* chore: :arrow_up: Update ggml-org/llama.cpp to `e75ba4c0434eb759eb7ff74e034ebe729053e575` by @localai-bot in https://github.com/mudler/LocalAI/pull/5783
* chore(bark-cpp): generalize and move to bark-cpp by @mudler in https://github.com/mudler/LocalAI/pull/5786
* chore: :arrow_up: Update PABannier/bark.cpp to `5d5be84f089ab9ea53b7a793f088d3fbf7247495` by @localai-bot in https://github.com/mudler/LocalAI/pull/4786
* chore: :arrow_up: Update ggml-org/llama.cpp to `bee28421be25fd447f61cb6db64d556cbfce32ec` by @localai-bot in https://github.com/mudler/LocalAI/pull/5788
* chore: :arrow_up: Update ggml-org/llama.cpp to `ef797db357e44ecb7437fa9d22f4e1614104b342` by @localai-bot in https://github.com/mudler/LocalAI/pull/5795
* chore: :arrow_up: Update ggml-org/llama.cpp to `a0374a67e2924f2e845cdc59dd67d9a44065a89c` by @localai-bot in https://github.com/mudler/LocalAI/pull/5798
* chore: :arrow_up: Update ggml-org/llama.cpp to `6491d6e4f1caf0ad2221865b4249ae6938a6308c` by @localai-bot in https://github.com/mudler/LocalAI/pull/5801
* chore: :arrow_up: Update ggml-org/llama.cpp to `12f55c302b35cfe900b84c5fe67c262026af9c44` by @localai-bot in https://github.com/mudler/LocalAI/pull/5808
* chore: :arrow_up: Update ggml-org/whisper.cpp to `869335f2d58d04010535be9ae23a69a9da12a169` by @localai-bot in https://github.com/mudler/LocalAI/pull/5809
* chore: :arrow_up: Update ggml-org/llama.cpp to `6efcd65945a98cf6883cdd9de4c8ccd8c79d219a` by @localai-bot in https://github.com/mudler/LocalAI/pull/5817
* chore: :arrow_up: Update ggml-org/llama.cpp to `0b8855775c6b873931d40b77a5e42558aacbde52` by @localai-bot in https://github.com/mudler/LocalAI/pull/5830
* chore: :arrow_up: Update ggml-org/llama.cpp to `f5e96b368f1acc7f53c390001b936517c4d18999` by @localai-bot in https://github.com/mudler/LocalAI/pull/5835
* chore: :arrow_up: Update ggml-org/llama.cpp to `c31e60647def83d671bac5ab5b35579bf25d9aa1` by @localai-bot in https://github.com/mudler/LocalAI/pull/5840
* chore: :arrow_up: Update ggml-org/whisper.cpp to `3775c503d5133d3d8b99d7d062e87a54064b0eb8` by @localai-bot in https://github.com/mudler/LocalAI/pull/5841
* chore: :arrow_up: Update ggml-org/whisper.cpp to `a16da91365700f396da916d16a7f5a2ec99364b9` by @localai-bot in https://github.com/mudler/LocalAI/pull/5846
* chore: :arrow_up: Update ggml-org/llama.cpp to `982e347255723fe6d02e60ee30cfdd0559c884c5` by @localai-bot in https://github.com/mudler/LocalAI/pull/5845
* chore: :arrow_up: Update ggml-org/whisper.cpp to `032697b9a850dc2615555e2a93a683cc3dd58559` by @localai-bot in https://github.com/mudler/LocalAI/pull/5849
* chore: :arrow_up: Update ggml-org/llama.cpp to `bdca38376f7e8dd928defe01ce6a16218a64b040` by @localai-bot in https://github.com/mudler/LocalAI/pull/5850
* chore: :arrow_up: Update ggml-org/llama.cpp to `4a4f426944e79b79e389f9ed7b34831cb9b637ad` by @localai-bot in https://github.com/mudler/LocalAI/pull/5852
* chore: :arrow_up: Update ggml-org/llama.cpp to `496957e1cbcb522abc63aa18521036e40efce985` by @localai-bot in https://github.com/mudler/LocalAI/pull/5854
* chore: :arrow_up: Update ggml-org/llama.cpp to `d6fb3f6b49b27ef1c0f4cf5128e041f7e7dc03af` by @localai-bot in https://github.com/mudler/LocalAI/pull/5857
* chore(deps): bump securego/gosec from 2.22.5 to 2.22.7 by @dependabot[bot] in https://github.com/mudler/LocalAI/pull/5878
* chore: :arrow_up: Update richiejp/stable-diffusion.cpp to `10c6501bd05a697e014f1bee3a84e5664290c489` by @localai-bot in https://github.com/mudler/LocalAI/pull/5732
* fix(stablediffusion-cpp): Switch back to upstream and update by @richiejp in https://github.com/mudler/LocalAI/pull/5880
### Other Changes
* docs: :arrow_up: update docs version mudler/LocalAI by @localai-bot in https://github.com/mudler/LocalAI/pull/5752
* docs: :arrow_up: update docs version mudler/LocalAI by @localai-bot in https://github.com/mudler/LocalAI/pull/5775
* docs: :arrow_up: update docs version mudler/LocalAI by @localai-bot in https://github.com/mudler/LocalAI/pull/5781
* chore: :arrow_up: Update ggml-org/llama.cpp to `bf9087f59aab940cf312b85a67067ce33d9e365a` by @localai-bot in https://github.com/mudler/LocalAI/pull/5860
* chore: :arrow_up: Update ggml-org/llama.cpp to `a979ca22db0d737af1e548a73291193655c6be99` by @localai-bot in https://github.com/mudler/LocalAI/pull/5862
* chore: :arrow_up: Update ggml-org/llama.cpp to `2be60cbc2707359241c2784f9d2e30d8fc7cdabb` by @localai-bot in https://github.com/mudler/LocalAI/pull/5867
* chore: :arrow_up: Update ggml-org/whisper.cpp to `1f5cf0b2888402d57bb17b2029b2caa97e5f3baf` by @localai-bot in https://github.com/mudler/LocalAI/pull/5876
* chore: :arrow_up: Update ggml-org/llama.cpp to `6c9ee3b17e19dcc82ab93d52ae46fdd0226d4777` by @localai-bot in https://github.com/mudler/LocalAI/pull/5877
* chore: drop vllm for cuda 11 by @mudler in https://github.com/mudler/LocalAI/pull/5881
* chore: :arrow_up: Update ggml-org/llama.cpp to `acd6cb1c41676f6bbb25c2a76fa5abeb1719301e` by @localai-bot in https://github.com/mudler/LocalAI/pull/5882
* fix: rename Dockerfile.go --&gt; Dockerfile.golang to avoid IDE errors by @dave-gray101 in https://github.com/mudler/LocalAI/pull/5892
* chore(Makefile): drop unused targets by @mudler in https://github.com/mudler/LocalAI/pull/5893
* chore: :arrow_up: Update ggml-org/llama.cpp to `a86f52b2859dae4db5a7a0bbc0f1ad9de6b43ec6` by @localai-bot in https://github.com/mudler/LocalAI/pull/5894
* fix: untangle pkg and core by @dave-gray101 in https://github.com/mudler/LocalAI/pull/5896
* Update quickstart.md by @Shinrai in https://github.com/mudler/LocalAI/pull/5898

&lt;/details&gt;

&lt;h2&gt;New Contributors&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;@dedyf5 made their first contribution in https://github.com/mudler/LocalAI/pull/5770&lt;/li&gt;
&lt;li&gt;@Shinrai made their first contribution in https://github.com/mudler/LocalAI/pull/5898&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Full Changelog&lt;/strong&gt;: https://github.com/mudler/LocalAI/compare/v3.1.1...v3.2.0&lt;/p&gt;</content>
    <link href="https://github.com/mudler/LocalAI/releases/tag/v3.2.0" rel="alternate"/>
  </entry>
  <entry>
    <id>https://github.com/mudler/LocalAI/releases/tag/v3.2.1</id>
    <title>New release for LocalAI: v3.2.1</title>
    <updated>2025-07-25T10:39:36-04:00</updated>
    <author>
      <name>mudler/LocalAI</name>
    </author>
    <content>&lt;!-- Release notes generated using configuration in .github/release.yml at v3.2.1 --&gt;

&lt;h2&gt;What's Changed&lt;/h2&gt;
&lt;h3&gt;Bug fixes :bug:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;fix(install.sh): update to use the new binary naming by @mudler in https://github.com/mudler/LocalAI/pull/5903&lt;/li&gt;
&lt;li&gt;fix(backends gallery): pass-by backend galleries to the model service by @mudler in https://github.com/mudler/LocalAI/pull/5906&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Other Changes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;chore: :arrow_up: Update ggml-org/llama.cpp to &lt;code&gt;3f4fc97f1d745f1d5d3c853949503136d419e6de&lt;/code&gt; by @localai-bot in https://github.com/mudler/LocalAI/pull/5900&lt;/li&gt;
&lt;li&gt;chore: :arrow_up: Update leejet/stable-diffusion.cpp to &lt;code&gt;eed97a5e1d054f9c1e7ac01982ae480411d4157e&lt;/code&gt; by @localai-bot in https://github.com/mudler/LocalAI/pull/5901&lt;/li&gt;
&lt;li&gt;chore: :arrow_up: Update ggml-org/whisper.cpp to &lt;code&gt;7de8dd783f7b2eab56bff6bbc5d3369e34f0e77f&lt;/code&gt; by @localai-bot in https://github.com/mudler/LocalAI/pull/5902&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Full Changelog&lt;/strong&gt;: https://github.com/mudler/LocalAI/compare/v3.2.0...v3.2.1&lt;/p&gt;</content>
    <link href="https://github.com/mudler/LocalAI/releases/tag/v3.2.1" rel="alternate"/>
  </entry>
</feed>
